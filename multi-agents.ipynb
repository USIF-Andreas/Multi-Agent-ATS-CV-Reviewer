{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ğŸ¤– Multi-Agent ATS Resume Analyzer System\n",
    "\n",
    "**A Production-Ready Multi-Agent System for ATS Resume Screening**\n",
    "\n",
    "This notebook implements a complete **Applicant Tracking System (ATS)** powered by **8 specialized AI agents** that work together to analyze, score, and provide feedback on resumes against job descriptions.\n",
    "\n",
    "### How It Works\n",
    "Each agent has a distinct role and personality â€” from deterministic extractors to semantic reasoners â€” mimicking how a real hiring pipeline operates:\n",
    "\n",
    "| # | Agent | Type | Role |\n",
    "|---|-------|------|------|\n",
    "| 1 | **CV Parsing Agent** | Extractor (Tool-augmented) | Extracts structured data (skills, experience, education) from PDF/DOCX/TXT resumes |\n",
    "| 2 | **Job Description Understanding Agent** | Semantic Analyst | Analyzes job postings to extract must-have/nice-to-have skills using NLP + knowledge base |\n",
    "| 3 | **Keyword Matching Agent** | Rule-Based Evaluator | Simulates traditional ATS keyword scoring with weighted sub-scores |\n",
    "| 4 | **Semantic Similarity Agent** | Semantic Reasoning | Goes beyond keywords â€” uses sentence embeddings to find synonym matches |\n",
    "| 5 | **Formatting Validator Agent** | Validator / Inspector | Checks for ATS-unfriendly formatting (tables, columns, images, icons) |\n",
    "| 6 | **Bias & Compliance Agent** | Policy / Safety | Flags age, gender, nationality info to ensure ethical screening |\n",
    "| 7 | **Feedback & Rewrite Agent** | Generative Coach | Provides actionable bullet-point suggestions for missing skills |\n",
    "| 8 | **Orchestrator Agent** | Manager | Coordinates all agents, resolves conflicts, and produces the final report |\n",
    "\n",
    "### Tech Stack\n",
    "- **NLP**: spaCy (`en_core_web_sm`) for entity recognition and text processing\n",
    "- **Embeddings**: Sentence-Transformers (`all-MiniLM-L6-v2`) for semantic similarity\n",
    "- **PDF Parsing**: PyPDF2 + pdfplumber for text and visual layout extraction\n",
    "- **API**: FastAPI + ngrok for production deployment\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ“¦ Installation & Setup\n",
    "\n",
    "Install all required Python packages and download the spaCy English language model. These are the core dependencies:\n",
    "- **PyPDF2** / **python-docx**: Parse PDF and DOCX resume files\n",
    "- **spaCy**: NLP pipeline for tokenization, entity recognition, and text processing\n",
    "- **sentence-transformers**: Pre-trained transformer models for computing semantic similarity between texts\n",
    "- **openai** / **anthropic**: (Optional) LLM API clients for future generative enhancements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-14T00:13:48.718837Z",
     "iopub.status.busy": "2026-02-14T00:13:48.718078Z",
     "iopub.status.idle": "2026-02-14T00:14:07.311961Z",
     "shell.execute_reply": "2026-02-14T00:14:07.311132Z",
     "shell.execute_reply.started": "2026-02-14T00:13:48.718804Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m232.6/232.6 kB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m253.0/253.0 kB\u001b[0m \u001b[31m17.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m405.9/405.9 kB\u001b[0m \u001b[31m27.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting en-core-web-sm==3.8.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.8.0/en_core_web_sm-3.8.0-py3-none-any.whl (12.8 MB)\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m77.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m\n",
      "\u001b[?25h\u001b[38;5;2mâœ” Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n",
      "\u001b[38;5;3mâš  Restart to reload dependencies\u001b[0m\n",
      "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
      "order to load all the package's dependencies. You can do this by selecting the\n",
      "'Restart kernel' or 'Restart runtime' option.\n"
     ]
    }
   ],
   "source": [
    "# Install required packages:\n",
    "# - PyPDF2: Reads text from PDF files\n",
    "# - python-docx: Reads text from Microsoft Word (.docx) files\n",
    "# - spacy: Industrial-strength NLP library for text processing\n",
    "# - sentence-transformers: Provides pre-trained models for computing dense vector embeddings\n",
    "# - openai, anthropic: Optional API clients for LLM-based enhancements\n",
    "!pip install -q PyPDF2 python-docx spacy sentence-transformers openai anthropic\n",
    "\n",
    "# Download the small English language model for spaCy (tokenization, POS tagging, NER)\n",
    "!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-14T00:14:07.313967Z",
     "iopub.status.busy": "2026-02-14T00:14:07.313736Z",
     "iopub.status.idle": "2026-02-14T00:14:45.469278Z",
     "shell.execute_reply": "2026-02-14T00:14:45.468545Z",
     "shell.execute_reply.started": "2026-02-14T00:14:07.313941Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-02-14 00:14:24.461043: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1771028064.676212      55 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1771028064.742896      55 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1771028065.314634      55 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1771028065.314689      55 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1771028065.314692      55 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1771028065.314694      55 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f2b3647471754142a84fb72f251e2a94",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "164ec6deefa04b47a96fd18efc1fb806",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config_sentence_transformers.json:   0%|          | 0.00/116 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7a161cabed294cd9b1e881df3b51fdc7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "115ddb1b921d44e4ac9e8ca96a30c16b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2ced0a0c97b54d65985268f487dbf881",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/612 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "42a6d5fcb3f54b1a801d33142380a93f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/90.9M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e174a20e9f78472c9a4b8240f9ab1c57",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/350 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "486b832c1db24fbc9f4952dcb805a788",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4c9618e217c84c6eb7c96a5840bac7ae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "30437500f7ff42aa82b01cca0ef92375",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3c45db8824934ae2a247b335ec4a8ad8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… All libraries loaded successfully\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# IMPORT LIBRARIES & LOAD NLP MODELS\n",
    "# ============================================================================\n",
    "# This cell imports all core libraries used throughout the notebook and\n",
    "# initializes two global NLP models that are shared across multiple agents:\n",
    "#   1. spaCy 'en_core_web_sm' â€” used for tokenization, part-of-speech tagging,\n",
    "#      and named entity recognition in the CV Parsing Agent.\n",
    "#   2. SentenceTransformer 'all-MiniLM-L6-v2' â€” a lightweight transformer model\n",
    "#      that encodes text into 384-dim vectors for cosine similarity comparisons.\n",
    "#      Used by the JD Agent, ATS Scorer, Semantic Agent, and Feedback Agent.\n",
    "# ============================================================================\n",
    "\n",
    "import re                                          # Regex for pattern extraction (emails, phones, skills)\n",
    "import json                                        # JSON serialization for report output\n",
    "import PyPDF2                                      # PDF text extraction\n",
    "import docx                                        # DOCX text extraction\n",
    "import spacy                                       # NLP pipeline (tokenization, NER)\n",
    "from sentence_transformers import SentenceTransformer, util  # Semantic similarity via embeddings\n",
    "from typing import Dict, List, Tuple, Optional     # Type hints for function signatures\n",
    "from dataclasses import dataclass, asdict          # Structured data containers (auto __init__, __repr__)\n",
    "from enum import Enum                              # Enumeration for decision outcomes (PASS/BORDERLINE/REJECT)\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')                  # Suppress transformer/deprecation warnings\n",
    "\n",
    "# Load NLP models (these are used globally by multiple agents)\n",
    "nlp = spacy.load('en_core_web_sm')                            # spaCy small English model\n",
    "embedding_model = SentenceTransformer('all-MiniLM-L6-v2')     # Sentence embedding model (384-dim vectors)\n",
    "\n",
    "print(\"âœ… All libraries loaded successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ“‹ Data Structures\n",
    "\n",
    "This cell defines all the **dataclasses** (structured data containers) used to pass information between agents. Each dataclass represents a specific piece of the analysis pipeline:\n",
    "\n",
    "| Dataclass | Purpose |\n",
    "|-----------|---------|\n",
    "| `PersonalInfo` | Stores contact details extracted from the CV (name, email, phone, location, LinkedIn) |\n",
    "| `Experience` | A single work experience entry (title, company, duration, bullet descriptions) |\n",
    "| `Education` | A single education entry (degree, institution, year, GPA) |\n",
    "| `ParsedCV` | The complete structured output of Agent 1 â€” all extracted CV data |\n",
    "| `JobRequirements` | The structured output of Agent 2 â€” extracted job requirements |\n",
    "| `ATSScore` | The scoring output of Agent 3 â€” overall + sub-scores with breakdowns |\n",
    "| `SemanticAnalysis` | The output of Agent 4 â€” similarity scores and semantic skill matches |\n",
    "| `FormatIssues` | The output of Agent 5 â€” ATS compatibility flags and recommendations |\n",
    "| `BiasFlags` | The output of Agent 6 â€” detected bias indicators and compliance status |\n",
    "| `Feedback` | The output of Agent 7 â€” suggested rewrites, missing keywords, priorities |\n",
    "| `Decision` | Enum with three outcomes: `PASS`, `BORDERLINE`, `REJECT` |\n",
    "| `FinalReport` | The combined output of Agent 8 â€” aggregates all agent outputs into one report |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-14T00:14:45.471162Z",
     "iopub.status.busy": "2026-02-14T00:14:45.470423Z",
     "iopub.status.idle": "2026-02-14T00:14:45.488136Z",
     "shell.execute_reply": "2026-02-14T00:14:45.487586Z",
     "shell.execute_reply.started": "2026-02-14T00:14:45.471137Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Data structures defined\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# DATA MODELS (DATACLASSES)\n",
    "# ============================================================================\n",
    "# These dataclasses define the schema for all inter-agent communication.\n",
    "# Using @dataclass gives us automatic __init__, __repr__, and asdict() support,\n",
    "# making it easy to serialize results to JSON for the API.\n",
    "# ============================================================================\n",
    "\n",
    "# --- CV Extraction Models ---\n",
    "\n",
    "@dataclass\n",
    "class PersonalInfo:\n",
    "    \"\"\"Contact details extracted from the resume header.\"\"\"\n",
    "    name: str = \"\"          # Candidate's full name\n",
    "    email: str = \"\"         # Email address (extracted via regex)\n",
    "    phone: str = \"\"         # Phone number (extracted via regex)\n",
    "    location: str = \"\"      # City/State/Country\n",
    "    linkedin: str = \"\"      # LinkedIn profile URL\n",
    "\n",
    "@dataclass\n",
    "class Experience:\n",
    "    \"\"\"A single work experience entry from the CV.\"\"\"\n",
    "    title: str              # Job title (e.g., \"Senior Software Engineer\")\n",
    "    company: str            # Company name\n",
    "    duration: str           # Date range (e.g., \"2021 - Present\")\n",
    "    description: List[str]  # Bullet-point descriptions of responsibilities/achievements\n",
    "\n",
    "@dataclass\n",
    "class Education:\n",
    "    \"\"\"A single education entry from the CV.\"\"\"\n",
    "    degree: str             # Degree name (e.g., \"B.S. in Computer Science\")\n",
    "    institution: str        # University/college name\n",
    "    year: str               # Graduation year\n",
    "    gpa: str = \"\"           # GPA (optional)\n",
    "\n",
    "@dataclass\n",
    "class ParsedCV:\n",
    "    \"\"\"Complete structured output from the CV Parsing Agent (Agent 1).\"\"\"\n",
    "    personal_info: PersonalInfo     # Contact information\n",
    "    skills: List[str]               # List of extracted technical/soft skills\n",
    "    experience: List[Experience]    # Work experience entries\n",
    "    education: List[Education]      # Education entries\n",
    "    projects: List[str]             # Project descriptions (if found)\n",
    "    certifications: List[str]       # Certifications (if found)\n",
    "    raw_text: str                   # Cleaned/normalized full text of the CV\n",
    "\n",
    "# --- Job Description Models ---\n",
    "\n",
    "@dataclass\n",
    "class JobRequirements:\n",
    "    \"\"\"Structured output from the Job Description Agent (Agent 2).\"\"\"\n",
    "    must_have_skills: List[str]     # Required/mandatory skills\n",
    "    nice_to_have_skills: List[str]  # Preferred/bonus skills\n",
    "    years_of_experience: int        # Minimum years of experience required\n",
    "    tools_technologies: List[str]   # Combined list of all tools/technologies\n",
    "    soft_skills: List[str]          # Soft skills mentioned (communication, teamwork, etc.)\n",
    "    job_title: str                  # Job title from the posting\n",
    "    raw_text: str                   # Original job description text\n",
    "\n",
    "# --- Scoring & Analysis Models ---\n",
    "\n",
    "@dataclass\n",
    "class ATSScore:\n",
    "    \"\"\"Scoring output from the ATS Scoring Agent (Agent 3).\"\"\"\n",
    "    overall_score: float                # Weighted aggregate score (0-100)\n",
    "    skill_match_score: float            # % of must-have skills matched\n",
    "    experience_score: float             # Experience years vs. requirement\n",
    "    title_similarity_score: float       # Semantic similarity between CV titles and JD title\n",
    "    missing_must_have: List[str]        # Must-have skills NOT found in the CV\n",
    "    matched_skills: List[str]           # All skills successfully matched\n",
    "    breakdown: Dict[str, float]         # Detailed sub-score breakdown\n",
    "\n",
    "@dataclass\n",
    "class SemanticAnalysis:\n",
    "    \"\"\"Output from the Semantic Similarity Agent (Agent 4).\"\"\"\n",
    "    cv_jd_similarity: float                             # Overall cosine similarity between CV and JD text\n",
    "    skill_semantic_matches: Dict[str, List[Tuple[str, float]]]  # JD skill â†’ [(CV skill, similarity score)]\n",
    "    experience_relevance: float                         # How relevant CV experience is to JD responsibilities\n",
    "\n",
    "# --- Validation & Compliance Models ---\n",
    "\n",
    "@dataclass\n",
    "class FormatIssues:\n",
    "    \"\"\"Output from the Formatting Validator Agent (Agent 5).\"\"\"\n",
    "    has_tables: bool                # Whether the document contains tables\n",
    "    has_columns: bool               # Whether a multi-column layout was detected\n",
    "    has_images: bool                # Whether images/graphics are embedded\n",
    "    has_icons: bool                 # Whether special Unicode icons/emojis are present\n",
    "    parsing_risks: List[str]        # List of identified ATS parsing risks\n",
    "    ats_friendly: bool              # Overall ATS compatibility verdict\n",
    "    recommendations: List[str]     # Actionable recommendations to fix issues\n",
    "\n",
    "@dataclass\n",
    "class BiasFlags:\n",
    "    \"\"\"Output from the Bias & Compliance Agent (Agent 6).\"\"\"\n",
    "    age_indicators: List[str]           # Detected age-related info (birth year, \"years old\")\n",
    "    gender_bias: List[str]              # Detected gender-specific language\n",
    "    nationality_references: List[str]   # Detected nationality/ethnicity references\n",
    "    photo_detected: bool                # Whether a photo/headshot was found\n",
    "    compliance_issues: List[str]        # Summarized compliance problems\n",
    "    is_compliant: bool                  # Overall compliance verdict\n",
    "\n",
    "# --- Feedback & Decision Models ---\n",
    "\n",
    "@dataclass\n",
    "class Feedback:\n",
    "    \"\"\"Output from the Feedback & Rewrite Agent (Agent 7).\"\"\"\n",
    "    skill_rewording: Dict[str, str]     # Original skill â†’ suggested rewording\n",
    "    stronger_bullets: List[str]         # Suggestions to strengthen weak bullet points\n",
    "    missing_keywords: List[str]         # Keywords missing from the CV\n",
    "    rewritten_sections: Dict[str, str]  # Section name â†’ rewritten content\n",
    "    improvement_priority: List[str]     # Prioritized list of improvements\n",
    "\n",
    "class Decision(Enum):\n",
    "    \"\"\"Final screening decision â€” used by the Orchestrator Agent.\"\"\"\n",
    "    PASS = \"pass\"               # Candidate meets requirements â†’ proceed to interview\n",
    "    BORDERLINE = \"borderline\"   # Partial match â†’ needs manual review\n",
    "    REJECT = \"reject\"           # Insufficient match â†’ does not meet minimum requirements\n",
    "\n",
    "@dataclass\n",
    "class FinalReport:\n",
    "    \"\"\"Combined output from the Orchestrator Agent (Agent 8) â€” the final deliverable.\"\"\"\n",
    "    decision: Decision                      # PASS / BORDERLINE / REJECT\n",
    "    ats_score: ATSScore                     # Agent 3 output\n",
    "    semantic_analysis: SemanticAnalysis      # Agent 4 output\n",
    "    format_issues: FormatIssues             # Agent 5 output\n",
    "    bias_flags: BiasFlags                   # Agent 6 output\n",
    "    feedback: Feedback                      # Agent 7 output\n",
    "    improvement_checklist: List[str]        # Aggregated actionable checklist\n",
    "    summary: str                            # Executive summary string\n",
    "\n",
    "print(\"âœ… Data structures defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ¤– Agent 1: CV Parsing Agent (Extractor Agent)\n",
    "\n",
    "**Agent Type:** Tool-augmented Extractor  \n",
    "**Role:** Extract structured data from resumes (PDF, DOCX, TXT)  \n",
    "**Characteristics:** Deterministic, high accuracy, no creativity â€” purely rule-based extraction\n",
    "\n",
    "This agent is the **entry point** of the pipeline. It takes a raw resume file and outputs a `ParsedCV` dataclass containing:\n",
    "- **Personal Info**: Email and phone extracted via regex patterns\n",
    "- **Skills**: Identified using a keyword list + regex-based skills section parsing\n",
    "- **Experience**: Job titles, companies, durations, and bullet descriptions parsed from the \"Experience\" section\n",
    "- **Education**: Degree, institution, and graduation year extracted from the \"Education\" section\n",
    "- **Normalized Text**: Cleaned text with bullets, icons, and extra whitespace removed\n",
    "\n",
    "**Key Methods:**\n",
    "- `parse_pdf()` / `parse_docx()` â€” File format handlers using PyPDF2 and python-docx\n",
    "- `extract_email()` / `extract_phone()` â€” Regex-based contact extraction\n",
    "- `extract_skills()` â€” Combines keyword matching with skills-section parsing\n",
    "- `extract_experience()` / `extract_education()` â€” Section-based structured extraction\n",
    "- `normalize_text()` â€” Strips formatting artifacts for downstream processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-14T00:14:45.490297Z",
     "iopub.status.busy": "2026-02-14T00:14:45.490055Z",
     "iopub.status.idle": "2026-02-14T00:14:45.537411Z",
     "shell.execute_reply": "2026-02-14T00:14:45.536827Z",
     "shell.execute_reply.started": "2026-02-14T00:14:45.490279Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… CV Parsing Agent initialized\n"
     ]
    }
   ],
   "source": [
    "class CVParsingAgent:\n",
    "    \"\"\"\n",
    "    Agent Type: Extractor Agent (Tool-augmented)\n",
    "    Role: Extract structured data from CV\n",
    "    Characteristics: Deterministic, high accuracy, no creativity\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.nlp = nlp\n",
    "    \n",
    "    def parse_pdf(self, pdf_path: str) -> str:\n",
    "        \"\"\"Extract text from PDF\"\"\"\n",
    "        text = \"\"\n",
    "        try:\n",
    "            with open(pdf_path, 'rb') as file:\n",
    "                pdf_reader = PyPDF2.PdfReader(file)\n",
    "                for page in pdf_reader.pages:\n",
    "                    text += page.extract_text()\n",
    "        except Exception as e:\n",
    "            print(f\"Error parsing PDF: {e}\")\n",
    "        return text\n",
    "    \n",
    "    def parse_docx(self, docx_path: str) -> str:\n",
    "        \"\"\"Extract text from DOCX\"\"\"\n",
    "        text = \"\"\n",
    "        try:\n",
    "            doc = docx.Document(docx_path)\n",
    "            for paragraph in doc.paragraphs:\n",
    "                text += paragraph.text + \"\\n\"\n",
    "        except Exception as e:\n",
    "            print(f\"Error parsing DOCX: {e}\")\n",
    "        return text\n",
    "    \n",
    "    def extract_email(self, text: str) -> str:\n",
    "        \"\"\"Extract email using regex\"\"\"\n",
    "        email_pattern = r'\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Z|a-z]{2,}\\b'\n",
    "        emails = re.findall(email_pattern, text)\n",
    "        return emails[0] if emails else \"\"\n",
    "    \n",
    "    def extract_phone(self, text: str) -> str:\n",
    "        \"\"\"Extract phone number\"\"\"\n",
    "        phone_pattern = r'\\+?\\d[\\d\\s\\-\\(\\)]{8,}\\d'\n",
    "        phones = re.findall(phone_pattern, text)\n",
    "        return phones[0] if phones else \"\"\n",
    "    \n",
    "    def extract_skills(self, text: str) -> List[str]:\n",
    "        \"\"\"Extract skills using pattern matching and NLP\"\"\"\n",
    "        # Common skill indicators\n",
    "        skill_keywords = [\n",
    "            'python', 'java', 'javascript', 'c\\+\\+', 'sql', 'nosql',\n",
    "            'machine learning', 'deep learning', 'nlp', 'computer vision',\n",
    "            'tensorflow', 'pytorch', 'keras', 'scikit-learn',\n",
    "            'aws', 'azure', 'gcp', 'docker', 'kubernetes',\n",
    "            'react', 'angular', 'vue', 'node.js', 'express',\n",
    "            'git', 'ci/cd', 'agile', 'scrum'\n",
    "        ]\n",
    "        \n",
    "        skills = []\n",
    "        text_lower = text.lower()\n",
    "        \n",
    "        for skill in skill_keywords:\n",
    "            if re.search(r'\\b' + skill + r'\\b', text_lower):\n",
    "                skills.append(skill.title())\n",
    "        \n",
    "        # Look for skills section\n",
    "        skills_section = re.search(r'skills?[:\\n](.*?)(?=\\n[A-Z]|$)', text, re.IGNORECASE | re.DOTALL)\n",
    "        if skills_section:\n",
    "            section_text = skills_section.group(1)\n",
    "            # Extract comma-separated or bullet-pointed skills\n",
    "            extracted = re.findall(r'[â€¢â—â—‹-]?\\s*([A-Za-z][A-Za-z\\s\\.\\+#]{2,}?)(?:[,;â€¢â—â—‹]|\\n|$)', section_text)\n",
    "            skills.extend([s.strip() for s in extracted if len(s.strip()) > 2])\n",
    "        \n",
    "        return list(set(skills))  # Remove duplicates\n",
    "    \n",
    "    def extract_experience(self, text: str) -> List[Experience]:\n",
    "        \"\"\"Extract work experience\"\"\"\n",
    "        experiences = []\n",
    "        \n",
    "        # Find experience section\n",
    "        exp_section = re.search(r'experience[:\\n](.*?)(?=education|projects|skills|$)', text, re.IGNORECASE | re.DOTALL)\n",
    "        if not exp_section:\n",
    "            return experiences\n",
    "        \n",
    "        section_text = exp_section.group(1)\n",
    "        \n",
    "        # Pattern for job entries (simplified)\n",
    "        job_pattern = r'([A-Z][^\\n]{5,60})\\s*[\\n|]\\s*([A-Z][^\\n]{5,60})\\s*[\\n|]\\s*([\\d]{4}.*?[\\d]{4}|present)'\n",
    "        matches = re.finditer(job_pattern, section_text, re.IGNORECASE)\n",
    "        \n",
    "        for match in matches:\n",
    "            title = match.group(1).strip()\n",
    "            company = match.group(2).strip()\n",
    "            duration = match.group(3).strip()\n",
    "            \n",
    "            # Extract bullet points after this job\n",
    "            start_pos = match.end()\n",
    "            next_job = re.search(job_pattern, section_text[start_pos:], re.IGNORECASE)\n",
    "            end_pos = start_pos + next_job.start() if next_job else len(section_text)\n",
    "            \n",
    "            job_desc = section_text[start_pos:end_pos]\n",
    "            bullets = re.findall(r'[â€¢â—â—‹-]\\s*(.+)', job_desc)\n",
    "            \n",
    "            experiences.append(Experience(\n",
    "                title=title,\n",
    "                company=company,\n",
    "                duration=duration,\n",
    "                description=bullets[:5]  # Limit to 5 bullets\n",
    "            ))\n",
    "        \n",
    "        return experiences\n",
    "    \n",
    "    def extract_education(self, text: str) -> List[Education]:\n",
    "        \"\"\"Extract education\"\"\"\n",
    "        education = []\n",
    "        \n",
    "        # Find education section\n",
    "        edu_section = re.search(r'education[:\\n](.*?)(?=experience|projects|skills|$)', text, re.IGNORECASE | re.DOTALL)\n",
    "        if not edu_section:\n",
    "            return education\n",
    "        \n",
    "        section_text = edu_section.group(1)\n",
    "        \n",
    "        # Pattern for education entries\n",
    "        edu_pattern = r'(bachelor|master|phd|b\\.?s\\.?|m\\.?s\\.?|ph\\.?d\\.?)[^\\n]{5,80}'\n",
    "        matches = re.finditer(edu_pattern, section_text, re.IGNORECASE)\n",
    "        \n",
    "        for match in matches:\n",
    "            degree_line = match.group(0).strip()\n",
    "            \n",
    "            # Extract year\n",
    "            year_match = re.search(r'(19|20)\\d{2}', degree_line)\n",
    "            year = year_match.group(0) if year_match else \"Unknown\"\n",
    "            \n",
    "            education.append(Education(\n",
    "                degree=degree_line,\n",
    "                institution=\"Unknown\",  # Could be improved with NER\n",
    "                year=year\n",
    "            ))\n",
    "        \n",
    "        return education\n",
    "    \n",
    "    def normalize_text(self, text: str) -> str:\n",
    "        \"\"\"Remove formatting, bullets, icons\"\"\"\n",
    "        # Remove bullet points\n",
    "        text = re.sub(r'[â€¢â—â—‹â– â–¡â–ªâ–«â—†â—‡]', '', text)\n",
    "        # Remove multiple spaces\n",
    "        text = re.sub(r'\\s+', ' ', text)\n",
    "        # Remove special characters except basic punctuation\n",
    "        text = re.sub(r'[^\\w\\s,.;:!?@#()\\-]', '', text)\n",
    "        return text.strip()\n",
    "    \n",
    "    def parse(self, file_path: str) -> ParsedCV:\n",
    "        \"\"\"Main parsing method\"\"\"\n",
    "        print(f\"ğŸ” Parsing CV: {file_path}\")\n",
    "        \n",
    "        # Extract text based on file type\n",
    "        if file_path.endswith('.pdf'):\n",
    "            raw_text = self.parse_pdf(file_path)\n",
    "        elif file_path.endswith('.docx'):\n",
    "            raw_text = self.parse_docx(file_path)\n",
    "        else:\n",
    "            with open(file_path, 'r', encoding='utf-8') as f:\n",
    "                raw_text = f.read()\n",
    "        \n",
    "        # Extract structured data\n",
    "        personal_info = PersonalInfo(\n",
    "            email=self.extract_email(raw_text),\n",
    "            phone=self.extract_phone(raw_text)\n",
    "        )\n",
    "        \n",
    "        skills = self.extract_skills(raw_text)\n",
    "        experience = self.extract_experience(raw_text)\n",
    "        education = self.extract_education(raw_text)\n",
    "        \n",
    "        normalized_text = self.normalize_text(raw_text)\n",
    "        \n",
    "        parsed_cv = ParsedCV(\n",
    "            personal_info=personal_info,\n",
    "            skills=skills,\n",
    "            experience=experience,\n",
    "            education=education,\n",
    "            projects=[],  # Could be extended\n",
    "            certifications=[],  # Could be extended\n",
    "            raw_text=normalized_text\n",
    "        )\n",
    "        \n",
    "        print(f\"âœ… Extracted {len(skills)} skills, {len(experience)} experiences, {len(education)} education entries\")\n",
    "        return parsed_cv\n",
    "\n",
    "# Test the agent\n",
    "print(\"âœ… CV Parsing Agent initialized\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ¤– Agent 2: Job Description Understanding Agent (Analyst Agent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-14T00:14:45.538534Z",
     "iopub.status.busy": "2026-02-14T00:14:45.538290Z",
     "iopub.status.idle": "2026-02-14T00:14:46.111492Z",
     "shell.execute_reply": "2026-02-14T00:14:46.110882Z",
     "shell.execute_reply.started": "2026-02-14T00:14:45.538501Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Semantic Job Agent Initialized\n"
     ]
    }
   ],
   "source": [
    "class JobDescriptionAgent:\n",
    "    \"\"\"\n",
    "    Agent Type: Semantic Analyst Agent\n",
    "    Role: Extract ONLY real technical skills using Knowledge Base + Semantic Validation\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.nlp = nlp\n",
    "        # The \"Brain\": Use the embedding model defined globally\n",
    "        self.model = embedding_model \n",
    "        \n",
    "        # 1. KNOWLEDGE BASE: Known technical skills (Whitelist)\n",
    "        self.known_skills = {\n",
    "            'python', 'java', 'c++', 'javascript', 'typescript', 'html', 'css', 'sql', 'nosql',\n",
    "            'react', 'angular', 'vue', 'node.js', 'django', 'flask', 'fastapi', 'spring',\n",
    "            'docker', 'kubernetes', 'aws', 'azure', 'gcp', 'terraform', 'ansible', 'jenkins',\n",
    "            'machine learning', 'deep learning', 'nlp', 'computer vision', 'pytorch', 'tensorflow',\n",
    "            'keras', 'scikit-learn', 'pandas', 'numpy', 'matplotlib', 'seaborn', 'opencv',\n",
    "            'git', 'github', 'gitlab', 'jira', 'agile', 'scrum', 'linux', 'unix', 'bash',\n",
    "            'redis', 'mongodb', 'postgresql', 'mysql', 'oracle', 'elasticsearch', 'kafka',\n",
    "            'spark', 'hadoop', 'hive', 'tableau', 'power bi', 'figma', 'llm', 'llms',\n",
    "            'transformers', 'huggingface', 'langchain', 'mlflow', 'mlops', 'ci/cd', 'rest api',\n",
    "            'graphql', 'grpc', 'bert', 'gpt', 'llama', 'generative ai', 'rag'\n",
    "        }\n",
    "\n",
    "        # 2. NOISE BLOCKLIST: Words that often get mistaken for skills\n",
    "        self.blocklist = {\n",
    "            'essential', 'required', 'mandatory', 'must', 'have', 'nice', 'preferred', 'desired',\n",
    "            'plus', 'strong', 'excellent', 'good', 'ability', 'proficient', 'experience', 'years',\n",
    "            'skills', 'knowledge', 'background', 'familiarity', 'hands-on', 'hands', 'expert',\n",
    "            'role', 'position', 'responsibilities', 'qualifications', 'requirements', 'about',\n",
    "            'team', 'work', 'environment', 'communication', 'collaboration', 'problem', 'solving',\n",
    "            'analytical', 'thinking', 'degree', 'computer', 'science', 'engineering', 'complex',\n",
    "            'concepts', 'technical', 'fast-paced', 'paced', 'tier', 'conferences', 'description',\n",
    "            'learning', 'frameworks', 'tools', 'platforms', 'apis', 'capability', 'like', 'with',\n",
    "            'and', 'or', 'for', 'the', 'soft', 'tuning', 'deployment', 'production', 'training',\n",
    "            'inference', 'fine-tuning', 'development', 'design', 'architecture', 'implementation',\n",
    "            'support', 'maintenance', 'testing', 'debugging', 'optimization', 'performance',\n",
    "            'scalability', 'security', 'privacy', 'compliance', 'regulations', 'standards',\n",
    "            'best practices', 'methodologies', 'processes', 'procedures', 'documentation',\n",
    "            'reporting', 'presentation', 'meetings', 'deadlines', 'priorities', 'stakeholders',\n",
    "            'clients', 'customers', 'users', 'business', 'requirements', 'needs', 'goals',\n",
    "            'objectives', 'strategies', 'tactics', 'plans', 'roadmaps', 'timelines', 'budgets',\n",
    "            'resources', 'costs', 'risks', 'issues', 'challenges', 'opportunities', 'trends',\n",
    "            'insights', 'analytics', 'data', 'information', 'systems', 'applications',\n",
    "            'software', 'hardware', 'networks', 'servers', 'databases', 'cloud', 'infrastructure',\n",
    "            'services', 'solutions', 'products', 'projects', 'programs', 'portfolios',\n",
    "            'operations', 'management', 'leadership', 'mentorship', 'coaching', 'training',\n",
    "            'hiring', 'recruiting', 'onboarding', 'performance', 'reviews', 'feedback',\n",
    "            'career', 'growth', 'development', 'salary', 'compensation', 'benefits', 'perks',\n",
    "            'culture', 'values', 'mission', 'vision', 'purpose', 'impact', 'innovation',\n",
    "            'creativity', 'collaboration', 'teamwork', 'communication', 'transparency',\n",
    "            'integrity', 'accountability', 'ownership', 'empowerment', 'diversity', 'inclusion',\n",
    "            'equity', 'belonging', 'wellness', 'balance', 'flexibility', 'remote', 'hybrid',\n",
    "            'office', 'location', 'relocation', 'travel', 'visa', 'sponsorship', 'citizenship',\n",
    "            'residency', 'authorization', 'clearance', 'background', 'check', 'drug', 'test',\n",
    "            'education', 'certification', 'license', 'degree', 'university', 'college', 'school',\n",
    "            'course', 'bootcamp', 'workshop', 'seminar', 'conference', 'icml', 'neurips', 'cvpr',\n",
    "            'iclr', 'aaai', 'ijcai', 'emnlp', 'naacl', 'eacl', 'acl', 'kdd', 'sigmod', 'vldb',\n",
    "            'icde', 'www', 'sigir', 'cikm', 'wsdm', 'recsys', 'chi', 'uist', 'cscw', 'hri',\n",
    "            'iros', 'icra', 'rss', 'corl', 'aistats', 'uai', 'colt', 'focs', 'stoc', 'soda',\n",
    "            'crypto', 'eurocrypt', 'asiacrypt', 'ches', 'sp', 'ccs', 'usenix', 'ndss', 'osdi',\n",
    "            'sosp', 'nsdi', 'sigcomm', 'mobicom', 'mobisys', 'infocom', 'globecom', 'icc',\n",
    "            'fast', 'of'\n",
    "        }\n",
    "        \n",
    "        # 3. Anchor embeddings for semantic validation\n",
    "        # We compare unknown words to these concepts. If similar, we keep them.\n",
    "        self.tech_anchors = [\"software technology tool\", \"programming language framework\", \"computer library\"]\n",
    "        self.anchor_embeddings = self.model.encode(self.tech_anchors)\n",
    "\n",
    "    def is_semantic_match(self, term: str, threshold: float = 0.35) -> bool:\n",
    "        \"\"\"Ask the AI: 'Is this word similar to a software tool?'\"\"\"\n",
    "        try:\n",
    "            term_emb = self.model.encode(term)\n",
    "            # Calculate similarity with anchors\n",
    "            scores = util.cos_sim(term_emb, self.anchor_embeddings)\n",
    "            max_score = float(scores.max())\n",
    "            \n",
    "            # If high similarity, it's likely a tech skill\n",
    "            return max_score > threshold\n",
    "        except:\n",
    "            return False\n",
    "\n",
    "    def validate_candidate(self, candidate: str) -> bool:\n",
    "        \"\"\"The Gatekeeper: Decide if a word is a real skill\"\"\"\n",
    "        clean_term = candidate.lower().strip().strip('.,()[]:')\n",
    "        \n",
    "        # Rule 1: Blocklist (Instant Fail)\n",
    "        if clean_term in self.blocklist or len(clean_term) < 2:\n",
    "            return False\n",
    "            \n",
    "        # Rule 2: Whitelist (Instant Pass)\n",
    "        if clean_term in self.known_skills:\n",
    "            return True\n",
    "            \n",
    "        # Rule 3: Heuristics (Regex patterns for versions/acronyms)\n",
    "        # e.g., \"GPT-4\", \"Llama-2\", \"C#\"\n",
    "        if re.search(r'\\d', clean_term) or clean_term in ['c#', '.net']:\n",
    "            return True\n",
    "            \n",
    "        # Rule 4: Semantic AI Check (The \"Maybe\" Pile)\n",
    "        # If it's a capitalized word not in blocklist, ask the AI\n",
    "        if candidate[0].isupper():\n",
    "            return self.is_semantic_match(clean_term)\n",
    "            \n",
    "        return False\n",
    "\n",
    "    def extract_skills_from_jd(self, text: str) -> Tuple[List[str], List[str]]:\n",
    "        must_have = []\n",
    "        nice_to_have = []\n",
    "        \n",
    "        # Find requirements section\n",
    "        req_section = re.search(r'(requirements?|qualifications?|skills?)[:\\n](.*?)(?=responsibilities|about|$)', \n",
    "                                text, re.IGNORECASE | re.DOTALL)\n",
    "        target_text = req_section.group(2) if req_section else text\n",
    "        \n",
    "        # Split by lines\n",
    "        lines = re.split(r'[\\nâ€¢â—â—‹-]', target_text)\n",
    "        \n",
    "        for line in lines:\n",
    "            # Skip empty lines\n",
    "            if not line.strip(): continue\n",
    "            \n",
    "            # Detect category (Must have vs Nice to have)\n",
    "            line_lower = line.lower()\n",
    "            is_nice = any(x in line_lower for x in ['preferred', 'plus', 'bonus', 'nice'])\n",
    "            \n",
    "            # 1. Extract potential candidates using Regex\n",
    "            # Captures: Acronyms (AWS), Capitalized Words (Python), and common Tech terms\n",
    "            candidates = re.findall(r'\\b[A-Z][a-zA-Z0-9\\+\\-\\.]{1,}\\b|\\b(?:python|java|sql|react|node|docker|linux)\\b', line)\n",
    "            \n",
    "            for cand in candidates:\n",
    "                # 2. Validate every candidate\n",
    "                if self.validate_candidate(cand):\n",
    "                    if is_nice:\n",
    "                        nice_to_have.append(cand)\n",
    "                    else:\n",
    "                        must_have.append(cand)\n",
    "\n",
    "        return list(set(must_have)), list(set(nice_to_have))\n",
    "\n",
    "    # Keep these standard helper methods\n",
    "    def extract_years_of_experience(self, text: str) -> int:\n",
    "        match = re.search(r'(\\d+)\\+?\\s*(?:-\\s*\\d+)?\\s*years?', text, re.IGNORECASE)\n",
    "        return int(match.group(1)) if match else 0\n",
    "\n",
    "    def extract_job_title(self, text: str) -> str:\n",
    "        lines = text.strip().split('\\n')\n",
    "        return lines[0].strip() if lines else \"Unknown\"\n",
    "\n",
    "    def extract_soft_skills(self, text: str) -> List[str]:\n",
    "        # Simple keyword match for soft skills\n",
    "        keywords = ['communication', 'teamwork', 'problem solving', 'leadership']\n",
    "        return [k.title() for k in keywords if k in text.lower()]\n",
    "\n",
    "    def analyze(self, jd_text: str) -> JobRequirements:\n",
    "        print(\"ğŸ” Analyzing Job Description with Semantic Agent...\")\n",
    "        must, nice = self.extract_skills_from_jd(jd_text)\n",
    "        years = self.extract_years_of_experience(jd_text)\n",
    "        \n",
    "        return JobRequirements(\n",
    "            must_have_skills=must,\n",
    "            nice_to_have_skills=nice,\n",
    "            years_of_experience=years,\n",
    "            tools_technologies=must + nice,\n",
    "            soft_skills=self.extract_soft_skills(jd_text),\n",
    "            job_title=self.extract_job_title(jd_text),\n",
    "            raw_text=jd_text\n",
    "        )\n",
    "\n",
    "# Re-initialize the agent\n",
    "print(\"âœ… Semantic Job Agent Initialized\")\n",
    "agent = JobDescriptionAgent()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ¤– Agent 3: Keyword Matching & ATS Scoring Agent (Rule-Based Evaluator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-14T00:14:46.112733Z",
     "iopub.status.busy": "2026-02-14T00:14:46.112441Z",
     "iopub.status.idle": "2026-02-14T00:14:46.127971Z",
     "shell.execute_reply": "2026-02-14T00:14:46.127371Z",
     "shell.execute_reply.started": "2026-02-14T00:14:46.112710Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… ATS Scoring Agent Updated with Semantic Title Matching\n"
     ]
    }
   ],
   "source": [
    "class ATSScoringAgent:\n",
    "    \"\"\"\n",
    "    Agent Type: Rule-Based Evaluator Agent\n",
    "    Role: Simulate ATS filtering logic with improved Semantic Title Matching\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        # We use the global embedding model for better title similarity\n",
    "        self.model = embedding_model \n",
    "        \n",
    "        # Scoring weights\n",
    "        self.weights = {\n",
    "            'must_have_match': 0.45,    # Increased weight for core skills\n",
    "            'nice_to_have_match': 0.10,\n",
    "            'experience_years': 0.25,\n",
    "            'title_similarity': 0.20\n",
    "        }\n",
    "    \n",
    "    def calculate_skill_match(self, cv_skills: List[str], required_skills: List[str]) -> Tuple[float, List[str], List[str]]:\n",
    "        if not required_skills:\n",
    "            return 0.0, [], [] \n",
    "        \n",
    "        cv_skills_lower = [s.lower() for s in cv_skills]\n",
    "        required_lower = [s.lower() for s in required_skills]\n",
    "        \n",
    "        matched = []\n",
    "        missing = []\n",
    "        \n",
    "        for req_skill in required_lower:\n",
    "            # Check for exact match or substring match (e.g. \"React\" in \"React.js\")\n",
    "            if any(req_skill in cv_s or cv_s in req_skill for cv_s in cv_skills_lower):\n",
    "                matched.append(req_skill)\n",
    "            else:\n",
    "                missing.append(req_skill)\n",
    "        \n",
    "        match_percentage = len(matched) / len(required_lower)\n",
    "        return match_percentage, matched, missing\n",
    "    \n",
    "    def calculate_experience_score(self, cv_experience: List[Experience], required_years: int) -> float:\n",
    "        if required_years == 0:\n",
    "            return 1.0\n",
    "        \n",
    "        total_years = 0\n",
    "        for exp in cv_experience:\n",
    "            # Smart check for years in string duration\n",
    "            years_match = re.search(r'(\\d+)\\s*year', exp.duration, re.IGNORECASE)\n",
    "            if years_match:\n",
    "                total_years += int(years_match.group(1))\n",
    "            elif \"present\" in exp.duration.lower() or \"current\" in exp.duration.lower():\n",
    "                total_years += 2 # Assume 2 years if it's a current ongoing role\n",
    "            else:\n",
    "                total_years += 1\n",
    "        \n",
    "        return min(1.0, total_years / required_years)\n",
    "    \n",
    "    def calculate_title_similarity(self, cv_titles: List[str], jd_title: str) -> float:\n",
    "        \"\"\"NEW: Uses AI Brain to compare titles instead of just counting words\"\"\"\n",
    "        if not cv_titles or not jd_title:\n",
    "            return 0.5\n",
    "        \n",
    "        # Encode the Job Title once\n",
    "        jd_emb = self.model.encode(jd_title.lower())\n",
    "        \n",
    "        max_sim = 0\n",
    "        for cv_title in cv_titles:\n",
    "            cv_emb = self.model.encode(cv_title.lower())\n",
    "            # Calculate cosine similarity between titles\n",
    "            sim = float(util.cos_sim(jd_emb, cv_emb))\n",
    "            max_sim = max(max_sim, sim)\n",
    "            \n",
    "        return max_sim\n",
    "\n",
    "    def detect_keyword_stuffing(self, cv_text: str, keywords: List[str]) -> bool:\n",
    "        cv_lower = cv_text.lower()\n",
    "        for keyword in keywords:\n",
    "            if cv_lower.count(keyword.lower()) > 6:\n",
    "                return True\n",
    "        return False\n",
    "    \n",
    "    def score(self, parsed_cv: ParsedCV, job_req: JobRequirements) -> ATSScore:\n",
    "        print(\"ğŸ“Š Calculating ATS Score (Semantic Title Mode)...\")\n",
    "        \n",
    "        # 1-4. Calculate sub-scores\n",
    "        must_have_score, matched_must, missing_must = self.calculate_skill_match(parsed_cv.skills, job_req.must_have_skills)\n",
    "        nice_have_score, matched_nice, _ = self.calculate_skill_match(parsed_cv.skills, job_req.nice_to_have_skills)\n",
    "        exp_score = self.calculate_experience_score(parsed_cv.experience, job_req.years_of_experience)\n",
    "        \n",
    "        cv_titles = [exp.title for exp in parsed_cv.experience]\n",
    "        title_score = self.calculate_title_similarity(cv_titles, job_req.job_title)\n",
    "        \n",
    "        # 5. Weighted aggregation\n",
    "        overall = (\n",
    "            must_have_score * self.weights['must_have_match'] +\n",
    "            nice_have_score * self.weights['nice_to_have_match'] +\n",
    "            exp_score * self.weights['experience_years'] +\n",
    "            title_score * self.weights['title_similarity']\n",
    "        )\n",
    "        \n",
    "        # 6. Keyword Stuffing Penalty\n",
    "        if self.detect_keyword_stuffing(parsed_cv.raw_text, job_req.must_have_skills):\n",
    "            overall *= 0.8\n",
    "            print(\"âš ï¸ Keyword stuffing detected - Penalty applied\")\n",
    "        \n",
    "        return ATSScore(\n",
    "            overall_score=round(overall * 100, 2),\n",
    "            skill_match_score=round(must_have_score * 100, 2),\n",
    "            experience_score=round(exp_score * 100, 2),\n",
    "            title_similarity_score=round(title_score * 100, 2),\n",
    "            missing_must_have=missing_must,\n",
    "            matched_skills=matched_must + matched_nice,\n",
    "            breakdown={\n",
    "                'must_have': must_have_score * 100,\n",
    "                'nice_have': nice_have_score * 100,\n",
    "                'experience': exp_score * 100,\n",
    "                'title': title_score * 100\n",
    "            }\n",
    "        )\n",
    "\n",
    "print(\"âœ… ATS Scoring Agent Updated with Semantic Title Matching\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ¤– Agent 4: Semantic Similarity Agent (Semantic Reasoning Agent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-14T00:14:46.129118Z",
     "iopub.status.busy": "2026-02-14T00:14:46.128818Z",
     "iopub.status.idle": "2026-02-14T00:14:46.144842Z",
     "shell.execute_reply": "2026-02-14T00:14:46.144079Z",
     "shell.execute_reply.started": "2026-02-14T00:14:46.129100Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Semantic Similarity Agent initialized\n"
     ]
    }
   ],
   "source": [
    "class SemanticSimilarityAgent:\n",
    "    \"\"\"\n",
    "    Agent Type: Semantic Reasoning Agent\n",
    "    Role: Go beyond keyword matching\n",
    "    Characteristics: Uses embeddings, handles synonyms, soft scoring\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.model = embedding_model\n",
    "    \n",
    "    def calculate_text_similarity(self, text1: str, text2: str) -> float:\n",
    "        \"\"\"Calculate cosine similarity between two texts\"\"\"\n",
    "        embeddings = self.model.encode([text1, text2])\n",
    "        similarity = util.cos_sim(embeddings[0], embeddings[1])\n",
    "        return float(similarity[0][0])\n",
    "    \n",
    "    def find_semantic_skill_matches(self, cv_skills: List[str], jd_skills: List[str], threshold: float = 0.6) -> Dict[str, List[Tuple[str, float]]]:\n",
    "        \"\"\"Find semantic matches between CV and JD skills\"\"\"\n",
    "        matches = {}\n",
    "        \n",
    "        if not cv_skills or not jd_skills:\n",
    "            return matches\n",
    "        \n",
    "        # Encode all skills\n",
    "        cv_embeddings = self.model.encode(cv_skills)\n",
    "        jd_embeddings = self.model.encode(jd_skills)\n",
    "        \n",
    "        # For each JD skill, find top CV matches\n",
    "        for i, jd_skill in enumerate(jd_skills):\n",
    "            similarities = util.cos_sim(jd_embeddings[i], cv_embeddings)[0]\n",
    "            \n",
    "            # Get matches above threshold\n",
    "            skill_matches = []\n",
    "            for j, sim in enumerate(similarities):\n",
    "                if float(sim) >= threshold:\n",
    "                    skill_matches.append((cv_skills[j], float(sim)))\n",
    "            \n",
    "            # Sort by similarity\n",
    "            skill_matches.sort(key=lambda x: x[1], reverse=True)\n",
    "            \n",
    "            if skill_matches:\n",
    "                matches[jd_skill] = skill_matches[:3]  # Top 3 matches\n",
    "        \n",
    "        return matches\n",
    "    \n",
    "    def calculate_experience_relevance(self, cv_experience: List[Experience], jd_text: str) -> float:\n",
    "        \"\"\"Calculate semantic relevance of experience to JD\"\"\"\n",
    "        if not cv_experience:\n",
    "            return 0.0\n",
    "        \n",
    "        # Combine all experience descriptions\n",
    "        cv_exp_text = \" \".join([\n",
    "            f\"{exp.title}. {' '.join(exp.description)}\" \n",
    "            for exp in cv_experience\n",
    "        ])\n",
    "        \n",
    "        # Extract responsibilities from JD\n",
    "        resp_section = re.search(r'responsibilities?[:\\n](.*?)(?=requirements|qualifications|$)', \n",
    "                                 jd_text, re.IGNORECASE | re.DOTALL)\n",
    "        \n",
    "        if resp_section:\n",
    "            jd_resp = resp_section.group(1)\n",
    "        else:\n",
    "            jd_resp = jd_text[:500]  # Use first 500 chars as fallback\n",
    "        \n",
    "        # Calculate similarity\n",
    "        similarity = self.calculate_text_similarity(cv_exp_text, jd_resp)\n",
    "        return similarity\n",
    "    \n",
    "    def analyze(self, parsed_cv: ParsedCV, job_req: JobRequirements) -> SemanticAnalysis:\n",
    "        \"\"\"Perform semantic analysis\"\"\"\n",
    "        print(\"ğŸ§  Performing Semantic Analysis...\")\n",
    "        \n",
    "        # Overall CV-JD similarity\n",
    "        cv_jd_sim = self.calculate_text_similarity(\n",
    "            parsed_cv.raw_text[:1000],  # First 1000 chars\n",
    "            job_req.raw_text[:1000]\n",
    "        )\n",
    "        \n",
    "        # Semantic skill matching\n",
    "        all_jd_skills = job_req.must_have_skills + job_req.nice_to_have_skills\n",
    "        skill_matches = self.find_semantic_skill_matches(\n",
    "            parsed_cv.skills, \n",
    "            all_jd_skills\n",
    "        )\n",
    "        \n",
    "        # Experience relevance\n",
    "        exp_relevance = self.calculate_experience_relevance(\n",
    "            parsed_cv.experience,\n",
    "            job_req.raw_text\n",
    "        )\n",
    "        \n",
    "        analysis = SemanticAnalysis(\n",
    "            cv_jd_similarity=round(cv_jd_sim, 3),\n",
    "            skill_semantic_matches=skill_matches,\n",
    "            experience_relevance=round(exp_relevance, 3)\n",
    "        )\n",
    "        \n",
    "        print(f\"âœ… CV-JD Similarity: {analysis.cv_jd_similarity:.2%}\")\n",
    "        print(f\"âœ… Found {len(skill_matches)} semantic skill matches\")\n",
    "        print(f\"âœ… Experience Relevance: {analysis.experience_relevance:.2%}\")\n",
    "        \n",
    "        return analysis\n",
    "\n",
    "print(\"âœ… Semantic Similarity Agent initialized\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ¤– Agent 5: Formatting & ATS Compatibility Agent (Validator Agent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-14T00:14:46.146337Z",
     "iopub.status.busy": "2026-02-14T00:14:46.145870Z",
     "iopub.status.idle": "2026-02-14T00:14:46.161538Z",
     "shell.execute_reply": "2026-02-14T00:14:46.160788Z",
     "shell.execute_reply.started": "2026-02-14T00:14:46.146318Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Formatting Validator Agent initialized\n"
     ]
    }
   ],
   "source": [
    "class FormattingValidatorAgent:\n",
    "    \"\"\"\n",
    "    Agent Type: Validator / Inspector Agent\n",
    "    Role: Detect ATS-unfriendly formatting\n",
    "    Characteristics: Rule-based, checklist-driven, flags risks\n",
    "    \"\"\"\n",
    "    \n",
    "    def check_for_tables(self, file_path: str) -> bool:\n",
    "        \"\"\"Check if document contains tables\"\"\"\n",
    "        if file_path.endswith('.docx'):\n",
    "            try:\n",
    "                doc = docx.Document(file_path)\n",
    "                return len(doc.tables) > 0\n",
    "            except:\n",
    "                return False\n",
    "        return False\n",
    "    \n",
    "    def check_for_columns(self, text: str) -> bool:\n",
    "        \"\"\"Detect multi-column layout (heuristic)\"\"\"\n",
    "        # If there are many lines with similar short length, might be columns\n",
    "        lines = text.split('\\n')\n",
    "        short_lines = [l for l in lines if 20 < len(l) < 40]\n",
    "        \n",
    "        # If >30% of lines are short and similar, likely columns\n",
    "        if len(short_lines) / len(lines) > 0.3:\n",
    "            return True\n",
    "        return False\n",
    "    \n",
    "    def check_for_images(self, file_path: str) -> bool:\n",
    "        \"\"\"Check for embedded images\"\"\"\n",
    "        if file_path.endswith('.docx'):\n",
    "            try:\n",
    "                doc = docx.Document(file_path)\n",
    "                # Check for images in document\n",
    "                for rel in doc.part.rels.values():\n",
    "                    if \"image\" in rel.target_ref:\n",
    "                        return True\n",
    "            except:\n",
    "                pass\n",
    "        return False\n",
    "    \n",
    "    def check_for_icons(self, text: str) -> bool:\n",
    "        \"\"\"Detect special characters/icons\"\"\"\n",
    "        # Unicode ranges for common icons\n",
    "        icon_ranges = [\n",
    "            (0x2600, 0x26FF),  # Miscellaneous Symbols\n",
    "            (0x2700, 0x27BF),  # Dingbats\n",
    "            (0x1F300, 0x1F9FF),  # Emoji\n",
    "        ]\n",
    "        \n",
    "        for char in text:\n",
    "            code = ord(char)\n",
    "            for start, end in icon_ranges:\n",
    "                if start <= code <= end:\n",
    "                    return True\n",
    "        return False\n",
    "    \n",
    "    def check_section_headers(self, text: str) -> List[str]:\n",
    "        \"\"\"Check for standard section headers\"\"\"\n",
    "        standard_sections = [\n",
    "            'experience', 'education', 'skills', 'summary',\n",
    "            'objective', 'certifications', 'projects'\n",
    "        ]\n",
    "        \n",
    "        missing_sections = []\n",
    "        text_lower = text.lower()\n",
    "        \n",
    "        for section in standard_sections:\n",
    "            if section not in text_lower:\n",
    "                missing_sections.append(section)\n",
    "        \n",
    "        return missing_sections\n",
    "    \n",
    "    def validate(self, file_path: str, parsed_cv: ParsedCV) -> FormatIssues:\n",
    "        \"\"\"Validate ATS compatibility\"\"\"\n",
    "        print(\"ğŸ” Checking ATS Compatibility...\")\n",
    "        \n",
    "        has_tables = self.check_for_tables(file_path)\n",
    "        has_columns = self.check_for_columns(parsed_cv.raw_text)\n",
    "        has_images = self.check_for_images(file_path)\n",
    "        has_icons = self.check_for_icons(parsed_cv.raw_text)\n",
    "        \n",
    "        parsing_risks = []\n",
    "        recommendations = []\n",
    "        \n",
    "        if has_tables:\n",
    "            parsing_risks.append(\"Tables detected - may cause parsing errors\")\n",
    "            recommendations.append(\"Remove tables and use simple lists instead\")\n",
    "        \n",
    "        if has_columns:\n",
    "            parsing_risks.append(\"Multi-column layout detected\")\n",
    "            recommendations.append(\"Use single-column layout for better ATS parsing\")\n",
    "        \n",
    "        if has_images:\n",
    "            parsing_risks.append(\"Images/graphics detected\")\n",
    "            recommendations.append(\"Remove images - ATS cannot read them\")\n",
    "        \n",
    "        if has_icons:\n",
    "            parsing_risks.append(\"Special characters/icons detected\")\n",
    "            recommendations.append(\"Replace icons with text (e.g., use 'Email:' instead of âœ‰)\")\n",
    "        \n",
    "        missing_sections = self.check_section_headers(parsed_cv.raw_text)\n",
    "        if 'experience' in missing_sections or 'education' in missing_sections:\n",
    "            parsing_risks.append(\"Missing standard section headers\")\n",
    "            recommendations.append(\"Add clear section headers: EXPERIENCE, EDUCATION, SKILLS\")\n",
    "        \n",
    "        ats_friendly = len(parsing_risks) == 0\n",
    "        \n",
    "        if ats_friendly:\n",
    "            recommendations.append(\"Resume is ATS-friendly! âœ…\")\n",
    "        \n",
    "        issues = FormatIssues(\n",
    "            has_tables=has_tables,\n",
    "            has_columns=has_columns,\n",
    "            has_images=has_images,\n",
    "            has_icons=has_icons,\n",
    "            parsing_risks=parsing_risks,\n",
    "            ats_friendly=ats_friendly,\n",
    "            recommendations=recommendations\n",
    "        )\n",
    "        \n",
    "        if ats_friendly:\n",
    "            print(\"âœ… Resume is ATS-friendly!\")\n",
    "        else:\n",
    "            print(f\"âš ï¸  Found {len(parsing_risks)} ATS compatibility issues\")\n",
    "        \n",
    "        return issues\n",
    "\n",
    "print(\"âœ… Formatting Validator Agent initialized\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ¤– Agent 6: Bias & Compliance Agent (Policy/Safety Agent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-14T00:14:46.162584Z",
     "iopub.status.busy": "2026-02-14T00:14:46.162314Z",
     "iopub.status.idle": "2026-02-14T00:14:46.176032Z",
     "shell.execute_reply": "2026-02-14T00:14:46.175328Z",
     "shell.execute_reply.started": "2026-02-14T00:14:46.162561Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Bias & Compliance Agent initialized\n"
     ]
    }
   ],
   "source": [
    "class BiasComplianceAgent:\n",
    "    \"\"\"\n",
    "    Agent Type: Policy / Safety Agent\n",
    "    Role: Ensure ethical and legal safety\n",
    "    Characteristics: Conservative, keyword detection, binary flags\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        # Age indicators\n",
    "        self.age_keywords = [\n",
    "            r'\\b(19|20)\\d{2}\\s*born\\b',  # Year of birth\n",
    "            r'\\bage\\s*:\\s*\\d+\\b',  # \"Age: 25\"\n",
    "            r'\\b\\d+\\s*years\\s*old\\b',  # \"25 years old\"\n",
    "        ]\n",
    "        \n",
    "        # Gender indicators (pronouns, gender-specific terms)\n",
    "        self.gender_keywords = [\n",
    "            r'\\b(he|she|his|her|him)\\b',\n",
    "            r'\\b(male|female|man|woman)\\b',\n",
    "        ]\n",
    "        \n",
    "        # Nationality/ethnicity indicators\n",
    "        self.nationality_keywords = [\n",
    "            r'\\bnationality\\s*:',\n",
    "            r'\\bethnicity\\s*:',\n",
    "            r'\\brace\\s*:',\n",
    "            r'\\bcitizenship\\s*:',\n",
    "        ]\n",
    "    \n",
    "    def detect_age_indicators(self, text: str) -> List[str]:\n",
    "        \"\"\"Detect age-related information\"\"\"\n",
    "        findings = []\n",
    "        text_lower = text.lower()\n",
    "        \n",
    "        for pattern in self.age_keywords:\n",
    "            matches = re.findall(pattern, text_lower, re.IGNORECASE)\n",
    "            if matches:\n",
    "                findings.extend(matches)\n",
    "        \n",
    "        return findings\n",
    "    \n",
    "    def detect_gender_bias(self, text: str) -> List[str]:\n",
    "        \"\"\"Detect gender-related information\"\"\"\n",
    "        findings = []\n",
    "        text_lower = text.lower()\n",
    "        \n",
    "        for pattern in self.gender_keywords:\n",
    "            matches = re.findall(pattern, text_lower, re.IGNORECASE)\n",
    "            if matches:\n",
    "                findings.extend(matches)\n",
    "        \n",
    "        return findings\n",
    "    \n",
    "    def detect_nationality_references(self, text: str) -> List[str]:\n",
    "        \"\"\"Detect nationality/ethnicity references\"\"\"\n",
    "        findings = []\n",
    "        text_lower = text.lower()\n",
    "        \n",
    "        for pattern in self.nationality_keywords:\n",
    "            matches = re.findall(pattern, text_lower, re.IGNORECASE)\n",
    "            if matches:\n",
    "                findings.append(pattern)\n",
    "        \n",
    "        return findings\n",
    "    \n",
    "    def detect_photo(self, file_path: str) -> bool:\n",
    "        \"\"\"Detect if resume contains a photo\"\"\"\n",
    "        if file_path.endswith('.docx'):\n",
    "            try:\n",
    "                doc = docx.Document(file_path)\n",
    "                for rel in doc.part.rels.values():\n",
    "                    if \"image\" in rel.target_ref.lower():\n",
    "                        # Check if it's likely a headshot (heuristic)\n",
    "                        return True\n",
    "            except:\n",
    "                pass\n",
    "        return False\n",
    "    \n",
    "    def check_compliance(self, file_path: str, parsed_cv: ParsedCV) -> BiasFlags:\n",
    "        \"\"\"Check for bias and compliance issues\"\"\"\n",
    "        print(\"âš–ï¸  Checking Bias & Compliance...\")\n",
    "        \n",
    "        age_indicators = self.detect_age_indicators(parsed_cv.raw_text)\n",
    "        gender_bias = self.detect_gender_bias(parsed_cv.raw_text)\n",
    "        nationality_refs = self.detect_nationality_references(parsed_cv.raw_text)\n",
    "        photo_detected = self.detect_photo(file_path)\n",
    "        \n",
    "        compliance_issues = []\n",
    "        \n",
    "        if age_indicators:\n",
    "            compliance_issues.append(\"Age information detected - remove to avoid age discrimination\")\n",
    "        \n",
    "        if gender_bias:\n",
    "            compliance_issues.append(\"Gender-specific language detected - use gender-neutral terms\")\n",
    "        \n",
    "        if nationality_refs:\n",
    "            compliance_issues.append(\"Nationality/ethnicity references detected - remove unless required\")\n",
    "        \n",
    "        if photo_detected:\n",
    "            compliance_issues.append(\"Photo detected - remove to avoid unconscious bias\")\n",
    "        \n",
    "        is_compliant = len(compliance_issues) == 0\n",
    "        \n",
    "        flags = BiasFlags(\n",
    "            age_indicators=age_indicators,\n",
    "            gender_bias=gender_bias,\n",
    "            nationality_references=nationality_refs,\n",
    "            photo_detected=photo_detected,\n",
    "            compliance_issues=compliance_issues,\n",
    "            is_compliant=is_compliant\n",
    "        )\n",
    "        \n",
    "        if is_compliant:\n",
    "            print(\"âœ… No compliance issues detected\")\n",
    "        else:\n",
    "            print(f\"âš ï¸  Found {len(compliance_issues)} compliance issues\")\n",
    "        \n",
    "        return flags\n",
    "\n",
    "print(\"âœ… Bias & Compliance Agent initialized\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ¤– Agent 7: Feedback & Rewrite Agent (Generative Coach Agent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-14T00:14:46.178694Z",
     "iopub.status.busy": "2026-02-14T00:14:46.178361Z",
     "iopub.status.idle": "2026-02-14T00:14:46.199280Z",
     "shell.execute_reply": "2026-02-14T00:14:46.198735Z",
     "shell.execute_reply.started": "2026-02-14T00:14:46.178675Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Feedback Agent Updated: Now Generates Real Content Suggestions\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from typing import List, Dict\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "class FeedbackAgent:\n",
    "    \"\"\"\n",
    "    Agent Type: Generative Coach\n",
    "    Role: Suggests REAL content (bullet points) for missing skills.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.model = embedding_model \n",
    "        \n",
    "        # 1. KNOWLEDGE BASE: Pre-written high-impact bullet points for common skills\n",
    "        # This allows the agent to give \"Real Suggestions\" you can copy-paste.\n",
    "        self.skill_content_db = {\n",
    "            'python': \"Developed automated scripts using Python (Pandas/NumPy) to process data, reducing manual effort by 40%.\",\n",
    "            'java': \"Architected scalable backend microservices using Java Spring Boot, handling 10k+ concurrent requests.\",\n",
    "            'react': \"Designed responsive frontend interfaces using React.js and Redux, improving user engagement time by 25%.\",\n",
    "            'node': \"Built RESTful APIs using Node.js/Express, optimizing database queries for 50% faster response times.\",\n",
    "            'node.js': \"Built RESTful APIs using Node.js/Express, optimizing database queries for 50% faster response times.\",\n",
    "            'kubernetes': \"Orchestrated containerized applications using Kubernetes (K8s), ensuring 99.9% uptime for production services.\",\n",
    "            'docker': \"Containerized legacy applications using Docker, reducing deployment cycle time from 2 days to 30 minutes.\",\n",
    "            'aws': \"Deployed fault-tolerant infrastructure on AWS (EC2, S3, Lambda), reducing monthly cloud costs by 15%.\",\n",
    "            'cloud': \"Managed cloud infrastructure migration, ensuring zero downtime and 100% data integrity.\",\n",
    "            'sql': \"Optimized complex SQL queries and stored procedures, improving report generation speed by 60%.\",\n",
    "            'machine learning': \"Trained predictive models using Scikit-Learn/TensorFlow, achieving 92% accuracy in forecasting.\",\n",
    "            'spark': \"Engineered real-time data pipelines using Apache Spark, processing TB-scale datasets for analytics.\",\n",
    "            'ci/cd': \"Implemented CI/CD pipelines using Jenkins/GitLab CI, enabling daily production releases.\",\n",
    "            'agile': \"Led daily stand-ups and sprint planning in an Agile/Scrum environment, improving team velocity by 20%.\"\n",
    "        }\n",
    "\n",
    "        self.action_verbs = ['Developed', 'Engineered', 'Deployed', 'Architected', 'Automated', 'Optimized', 'Led', 'Managed']\n",
    "\n",
    "    def generate_content_suggestion(self, missing_skill: str) -> str:\n",
    "        \"\"\"Finds or generates a bullet point for a missing skill\"\"\"\n",
    "        missing_lower = missing_skill.lower()\n",
    "        \n",
    "        # 1. Direct Knowledge Base Lookup\n",
    "        for key, bullet in self.skill_content_db.items():\n",
    "            if key in missing_lower or missing_lower in key:\n",
    "                return bullet\n",
    "                \n",
    "        # 2. Generic Fallback (if skill not in DB)\n",
    "        return f\"Integrated {missing_skill} into the development workflow to enhance system performance and scalability.\"\n",
    "\n",
    "    def detect_fluff(self, bullets: List[str], jd_text: str) -> List[str]:\n",
    "        if not bullets or not jd_text: return []\n",
    "        try:\n",
    "            jd_emb = self.model.encode(jd_text).reshape(1, -1)\n",
    "            bullet_embs = self.model.encode(bullets)\n",
    "            scores = cosine_similarity(bullet_embs, jd_emb).flatten()\n",
    "            return [bullets[i] for i, score in enumerate(scores) if score < 0.15 and len(bullets[i]) > 30]\n",
    "        except:\n",
    "            return []\n",
    "\n",
    "    def analyze_bullet_strength(self, bullet: str) -> str:\n",
    "        words = bullet.lower().split()\n",
    "        if not words or words[0] not in [v.lower() for v in self.action_verbs]:\n",
    "            return \"Start with an Action Verb\"\n",
    "        if not re.search(r'\\d+%|\\$\\d+|\\d+x|\\d+\\+', bullet):\n",
    "            return \"Add a Metric (e.g. 20%)\"\n",
    "        return \"Strong\"\n",
    "\n",
    "    def generate_feedback(self, parsed_cv: ParsedCV, job_req: JobRequirements, ats_score: ATSScore) -> Feedback:\n",
    "        print(\"ğŸ’¡ Feedback Agent: Generating REAL content suggestions...\")\n",
    "        \n",
    "        priorities = []\n",
    "        stronger_bullets = []\n",
    "        \n",
    "        # 1. REAL SUGGESTIONS for Missing Skills\n",
    "        if ats_score.missing_must_have:\n",
    "            priorities.append(f\"ğŸ”´ CRITICAL: Missing Keywords: {', '.join(ats_score.missing_must_have[:3])}\")\n",
    "            \n",
    "            for skill in ats_score.missing_must_have[:3]: # Suggest for top 3 missing\n",
    "                suggestion = self.generate_content_suggestion(skill)\n",
    "                # Add concrete advice to the list\n",
    "                priorities.append(f\"   ğŸ’¡ Suggestion for '{skill}': Add a bullet like -> \\\"{suggestion}\\\"\")\n",
    "\n",
    "        # 2. Fix Weak Bullets\n",
    "        all_bullets = [b for exp in parsed_cv.experience for b in exp.description]\n",
    "        for bullet in all_bullets[:5]:\n",
    "            issue = self.analyze_bullet_strength(bullet)\n",
    "            if issue != \"Strong\":\n",
    "                stronger_bullets.append(f\"Rewrite '{bullet[:30]}...' -> {issue}\")\n",
    "\n",
    "        return Feedback(\n",
    "            missing_keywords=ats_score.missing_must_have,\n",
    "            skill_rewording={},\n",
    "            stronger_bullets=stronger_bullets,\n",
    "            improvement_priority=priorities,\n",
    "            rewritten_sections={}\n",
    "        )\n",
    "\n",
    "print(\"âœ… Feedback Agent Updated: Now Generates Real Content Suggestions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-14T00:14:46.200278Z",
     "iopub.status.busy": "2026-02-14T00:14:46.200107Z",
     "iopub.status.idle": "2026-02-14T00:14:46.210930Z",
     "shell.execute_reply": "2026-02-14T00:14:46.210340Z",
     "shell.execute_reply.started": "2026-02-14T00:14:46.200262Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "from dataclasses import asdict\n",
    "\n",
    "def save_report_json(report, filename: str):\n",
    "    \"\"\"Saves the FinalReport object to a JSON file.\"\"\"\n",
    "    try:\n",
    "        # Convert dataclass to dict if it isn't already\n",
    "        if hasattr(report, '__dataclass_fields__'):\n",
    "            data = asdict(report)\n",
    "        else:\n",
    "            data = report\n",
    "            \n",
    "        with open(filename, 'w', encoding='utf-8') as f:\n",
    "            json.dump(data, f, indent=4, default=str)\n",
    "            \n",
    "        print(f\"âœ… Report successfully saved to: {filename}\")\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Error saving JSON: {e}\")\n",
    "\n",
    "def print_final_report(report):\n",
    "    \"\"\"Prints a formatted summary of the analysis to the console.\"\"\"\n",
    "    \n",
    "    # Handle both object and dict access\n",
    "    def get_attr(obj, key):\n",
    "        if isinstance(obj, dict):\n",
    "            return obj.get(key)\n",
    "        return getattr(obj, key, None)\n",
    "\n",
    "    decision = get_attr(report, 'decision')\n",
    "    score = get_attr(get_attr(report, 'ats_score'), 'overall_score')\n",
    "    summary = get_attr(report, 'summary')\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"ğŸ“‹ FINAL ATS ANALYSIS REPORT\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # 1. Executive Summary\n",
    "    print(f\"\\nğŸ”¹ DECISION: {decision}\")\n",
    "    if score:\n",
    "        print(f\"ğŸ”¹ OVERALL SCORE: {score:.1f}/100\")\n",
    "    print(f\"\\nğŸ“ SUMMARY:\\n{summary}\")\n",
    "    \n",
    "    # 2. Missing Skills (if any)\n",
    "    ats_score = get_attr(report, 'ats_score')\n",
    "    missing = get_attr(ats_score, 'missing_must_have')\n",
    "    if missing:\n",
    "        print(f\"\\nâš ï¸ MISSING CRITICAL SKILLS:\\n\" + \", \".join(missing))\n",
    "    \n",
    "    # 3. Improvement Checklist\n",
    "    checklist = get_attr(report, 'improvement_checklist')\n",
    "    if checklist:\n",
    "        print(\"\\nâœ… IMPROVEMENT CHECKLIST:\")\n",
    "        for item in checklist:\n",
    "            print(f\"  - {item}\")\n",
    "            \n",
    "    # 4. Visual Layout (if available)\n",
    "    visual = get_attr(report, 'visual_report')\n",
    "    if visual and isinstance(visual, dict):\n",
    "        pos = visual.get('skills_position', 'Unknown')\n",
    "        print(f\"\\nğŸ‘ï¸ VISUAL LAYOUT:\\n  - Skills Position: {pos}\")\n",
    "        if visual.get('issues'):\n",
    "            print(f\"  - Issues: {', '.join(visual['issues'])}\")\n",
    "\n",
    "    print(\"\\n\" + \"=\"*60 + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ¤– Auto-Improve "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-14T00:14:46.211875Z",
     "iopub.status.busy": "2026-02-14T00:14:46.211619Z",
     "iopub.status.idle": "2026-02-14T00:14:50.536531Z",
     "shell.execute_reply": "2026-02-14T00:14:50.535598Z",
     "shell.execute_reply.started": "2026-02-14T00:14:46.211845Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pdfplumber\n",
      "  Downloading pdfplumber-0.11.9-py3-none-any.whl.metadata (43 kB)\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m43.6/43.6 kB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting pdfminer.six==20251230 (from pdfplumber)\n",
      "  Downloading pdfminer_six-20251230-py3-none-any.whl.metadata (4.3 kB)\n",
      "Requirement already satisfied: Pillow>=9.1 in /usr/local/lib/python3.12/dist-packages (from pdfplumber) (11.3.0)\n",
      "Collecting pypdfium2>=4.18.0 (from pdfplumber)\n",
      "  Downloading pypdfium2-5.4.0-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (67 kB)\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m67.9/67.9 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: charset-normalizer>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from pdfminer.six==20251230->pdfplumber) (3.4.4)\n",
      "Requirement already satisfied: cryptography>=36.0.0 in /usr/local/lib/python3.12/dist-packages (from pdfminer.six==20251230->pdfplumber) (46.0.3)\n",
      "Requirement already satisfied: cffi>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from cryptography>=36.0.0->pdfminer.six==20251230->pdfplumber) (2.0.0)\n",
      "Requirement already satisfied: pycparser in /usr/local/lib/python3.12/dist-packages (from cffi>=2.0.0->cryptography>=36.0.0->pdfminer.six==20251230->pdfplumber) (2.23)\n",
      "Downloading pdfplumber-0.11.9-py3-none-any.whl (60 kB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m60.0/60.0 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pdfminer_six-20251230-py3-none-any.whl (6.6 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m6.6/6.6 MB\u001b[0m \u001b[31m66.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading pypdfium2-5.4.0-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.0 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m65.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: pypdfium2, pdfminer.six, pdfplumber\n",
      "Successfully installed pdfminer.six-20251230 pdfplumber-0.11.9 pypdfium2-5.4.0\n"
     ]
    }
   ],
   "source": [
    "!pip install pdfplumber"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-14T00:14:50.538922Z",
     "iopub.status.busy": "2026-02-14T00:14:50.538290Z",
     "iopub.status.idle": "2026-02-14T00:14:50.598962Z",
     "shell.execute_reply": "2026-02-14T00:14:50.598286Z",
     "shell.execute_reply.started": "2026-02-14T00:14:50.538890Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import pdfplumber\n",
    "import statistics\n",
    "\n",
    "class VisualLayoutAgent:\n",
    "    \"\"\"\n",
    "    Agent Type: Visual Analyst\n",
    "    Role: Inspects physical layout, font usage, and section placement.\n",
    "    \"\"\"\n",
    "    \n",
    "    def analyze_layout(self, file_path: str):\n",
    "        print(\"ğŸ‘ï¸ Visual Agent: Scanning document layout...\")\n",
    "        \n",
    "        report = {\n",
    "            \"font_names\": set(),\n",
    "            \"font_sizes\": [],\n",
    "            \"skills_position\": \"Not Found\", # Top, Middle, Bottom, or Not Found\n",
    "            \"layout_score\": 100,\n",
    "            \"issues\": []\n",
    "        }\n",
    "        \n",
    "        try:\n",
    "            with pdfplumber.open(file_path) as pdf:\n",
    "                full_text_height = 0\n",
    "                skills_y_coord = None\n",
    "                total_chars = 0\n",
    "                \n",
    "                # Check Page 1 (Critical for ATS)\n",
    "                first_page = pdf.pages[0]\n",
    "                height = first_page.height\n",
    "                width = first_page.width\n",
    "                \n",
    "                # Extract words with layout info\n",
    "                words = first_page.extract_words(extra_attrs=[\"fontname\", \"size\", \"top\"])\n",
    "                \n",
    "                for word in words:\n",
    "                    # Collect Font Stats\n",
    "                    report[\"font_names\"].add(word[\"fontname\"])\n",
    "                    report[\"font_sizes\"].append(float(word[\"size\"]))\n",
    "                    total_chars += len(word[\"text\"])\n",
    "                    \n",
    "                    # Check for \"Skills\" section header\n",
    "                    if \"skill\" in word[\"text\"].lower() and float(word[\"size\"]) > 11:\n",
    "                        # Ensure it's a header (usually larger or standalone)\n",
    "                        skills_y_coord = word[\"top\"]\n",
    "\n",
    "                # 1. Analyze Skills Placement\n",
    "                if skills_y_coord:\n",
    "                    # 0 is top, 'height' is bottom\n",
    "                    relative_pos = skills_y_coord / height\n",
    "                    if relative_pos < 0.3:\n",
    "                        report[\"skills_position\"] = \"Top (Excellent)\"\n",
    "                    elif relative_pos < 0.6:\n",
    "                        report[\"skills_position\"] = \"Middle (Good)\"\n",
    "                    else:\n",
    "                        report[\"skills_position\"] = \"Bottom (Risk)\"\n",
    "                        report[\"issues\"].append(\"Visual: 'Skills' section is too low. Move it to the top 30% of page 1.\")\n",
    "                        report[\"layout_score\"] -= 15\n",
    "                else:\n",
    "                    report[\"issues\"].append(\"Visual: Could not visually locate a distinct 'Skills' header.\")\n",
    "\n",
    "                # 2. Analyze Font Sizes\n",
    "                if report[\"font_sizes\"]:\n",
    "                    avg_size = statistics.mean(report[\"font_sizes\"])\n",
    "                    min_size = min(report[\"font_sizes\"])\n",
    "                    \n",
    "                    if min_size < 9:\n",
    "                        report[\"issues\"].append(f\"Visual: Text is too small ({min_size}pt). Minimum recommended is 10pt.\")\n",
    "                        report[\"layout_score\"] -= 10\n",
    "                    \n",
    "                    if len(report[\"font_names\"]) > 3:\n",
    "                        report[\"issues\"].append(f\"Visual: Too many font types detected ({len(report['font_names'])}). Keep it to 1 or 2.\")\n",
    "                        report[\"layout_score\"] -= 5\n",
    "\n",
    "                # 3. Analyze Clutter (Density)\n",
    "                # A standard readable page has ~2500-3000 chars max.\n",
    "                if total_chars > 3500:\n",
    "                    report[\"issues\"].append(\"Visual: Page is too dense/cluttered. Add more whitespace.\")\n",
    "                    report[\"layout_score\"] -= 10\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Visual Analysis Warning: {e}\")\n",
    "            report[\"issues\"].append(\"Could not perform visual analysis (file might not be a text-PDF).\")\n",
    "            \n",
    "        return report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ¤– Agent 8: Orchestrator Agent (Manager Agent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-14T00:14:50.600431Z",
     "iopub.status.busy": "2026-02-14T00:14:50.599844Z",
     "iopub.status.idle": "2026-02-14T00:14:50.614371Z",
     "shell.execute_reply": "2026-02-14T00:14:50.613704Z",
     "shell.execute_reply.started": "2026-02-14T00:14:50.600398Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Orchestrator Agent initialized\n"
     ]
    }
   ],
   "source": [
    "class OrchestratorAgent:\n",
    "    \"\"\"\n",
    "    Agent Type: Orchestrator / Manager Agent\n",
    "    Role: Coordinate all agents and produce final report\n",
    "    Characteristics: High-level reasoning, conflict resolution, final decision\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.cv_parser = CVParsingAgent()\n",
    "        self.jd_analyzer = JobDescriptionAgent()\n",
    "        self.ats_scorer = ATSScoringAgent()\n",
    "        self.semantic_analyzer = SemanticSimilarityAgent()\n",
    "        self.format_validator = FormattingValidatorAgent()\n",
    "        self.bias_checker = BiasComplianceAgent()\n",
    "        self.feedback_agent = FeedbackAgent()\n",
    "        \n",
    "    \n",
    "    def determine_decision(self, ats_score: ATSScore, semantic_analysis: SemanticAnalysis,\n",
    "                          format_issues: FormatIssues, bias_flags: BiasFlags) -> Decision:\n",
    "        \"\"\"Determine final pass/borderline/reject decision\"\"\"\n",
    "        \n",
    "        # Automatic reject conditions\n",
    "        if not bias_flags.is_compliant:\n",
    "            return Decision.REJECT\n",
    "        \n",
    "        if len(ats_score.missing_must_have) > 3:\n",
    "            return Decision.REJECT\n",
    "        \n",
    "        # Pass conditions\n",
    "        if (\n",
    "            ats_score.overall_score >= 75 and\n",
    "            semantic_analysis.cv_jd_similarity >= 0.6 and\n",
    "            format_issues.ats_friendly\n",
    "        ):\n",
    "            return Decision.PASS\n",
    "        \n",
    "        # Borderline conditions\n",
    "        if (\n",
    "            ats_score.overall_score >= 60 or\n",
    "            semantic_analysis.cv_jd_similarity >= 0.5\n",
    "        ):\n",
    "            return Decision.BORDERLINE\n",
    "        \n",
    "        # Default to reject\n",
    "        return Decision.REJECT\n",
    "    \n",
    "    def create_improvement_checklist(self, feedback: Feedback, format_issues: FormatIssues,\n",
    "                                    bias_flags: BiasFlags) -> List[str]:\n",
    "        \"\"\"Create actionable improvement checklist\"\"\"\n",
    "        checklist = []\n",
    "        \n",
    "        # From feedback\n",
    "        checklist.extend(feedback.improvement_priority)\n",
    "        \n",
    "        # From format issues\n",
    "        if not format_issues.ats_friendly:\n",
    "            checklist.extend([f\"ğŸŸ  {rec}\" for rec in format_issues.recommendations])\n",
    "        \n",
    "        # From bias/compliance\n",
    "        if not bias_flags.is_compliant:\n",
    "            checklist.extend([f\"ğŸ”´ {issue}\" for issue in bias_flags.compliance_issues])\n",
    "        \n",
    "        return checklist\n",
    "    \n",
    "    def generate_summary(self, decision: Decision, ats_score: ATSScore,\n",
    "                        semantic_analysis: SemanticAnalysis) -> str:\n",
    "        \"\"\"Generate executive summary\"\"\"\n",
    "        summaries = {\n",
    "            Decision.PASS: f\"âœ… **PASS** - Strong candidate with {ats_score.overall_score:.0f}/100 ATS score. \"\n",
    "                          f\"Resume shows {semantic_analysis.cv_jd_similarity:.0%} semantic match with job requirements. \"\n",
    "                          f\"Proceed to interview.\",\n",
    "            \n",
    "            Decision.BORDERLINE: f\"âš ï¸  **BORDERLINE** - Moderate fit with {ats_score.overall_score:.0f}/100 ATS score. \"\n",
    "                                f\"Resume shows {semantic_analysis.cv_jd_similarity:.0%} match. \"\n",
    "                                f\"Consider with improvements or manual review.\",\n",
    "            \n",
    "            Decision.REJECT: f\"âŒ **REJECT** - Insufficient match with {ats_score.overall_score:.0f}/100 ATS score. \"\n",
    "                            f\"Resume shows {semantic_analysis.cv_jd_similarity:.0%} match. \"\n",
    "                            f\"Candidate does not meet minimum requirements.\"\n",
    "        }\n",
    "        \n",
    "        return summaries[decision]\n",
    "    \n",
    "    def process(self, cv_file_path: str, jd_text: str) -> FinalReport:\n",
    "        \"\"\"\n",
    "        Main orchestration method - coordinates all agents\n",
    "        \"\"\"\n",
    "        print(\"\\n\" + \"=\"*70)\n",
    "        print(\"ğŸš€ STARTING ATS MULTI-AGENT ANALYSIS\")\n",
    "        print(\"=\"*70 + \"\\n\")\n",
    "        \n",
    "        # Step 1: Parse CV\n",
    "        print(\"[1/8] Running CV Parsing Agent...\")\n",
    "        parsed_cv = self.cv_parser.parse(cv_file_path)\n",
    "        \n",
    "        # Step 2: Analyze JD\n",
    "        print(\"\\n[2/8] Running Job Description Agent...\")\n",
    "        job_req = self.jd_analyzer.analyze(jd_text)\n",
    "        \n",
    "        # Step 3: ATS Scoring\n",
    "        print(\"\\n[3/8] Running ATS Scoring Agent...\")\n",
    "        ats_score = self.ats_scorer.score(parsed_cv, job_req)\n",
    "        \n",
    "        # Step 4: Semantic Analysis\n",
    "        print(\"\\n[4/8] Running Semantic Similarity Agent...\")\n",
    "        semantic_analysis = self.semantic_analyzer.analyze(parsed_cv, job_req)\n",
    "        \n",
    "        # Step 5: Format Validation\n",
    "        print(\"\\n[5/8] Running Formatting Validator Agent...\")\n",
    "        format_issues = self.format_validator.validate(cv_file_path, parsed_cv)\n",
    "        \n",
    "        # Step 6: Bias Check\n",
    "        print(\"\\n[6/8] Running Bias & Compliance Agent...\")\n",
    "        bias_flags = self.bias_checker.check_compliance(cv_file_path, parsed_cv)\n",
    "        \n",
    "        # Step 7: Generate Feedback\n",
    "        print(\"\\n[7/8] Running Feedback Agent...\")\n",
    "        feedback = self.feedback_agent.generate_feedback(parsed_cv, job_req, ats_score)\n",
    "        \n",
    "        # Step 8: Make Final Decision\n",
    "        print(\"\\n[8/8] Making Final Decision...\")\n",
    "        decision = self.determine_decision(ats_score, semantic_analysis, format_issues, bias_flags)\n",
    "        improvement_checklist = self.create_improvement_checklist(feedback, format_issues, bias_flags)\n",
    "        summary = self.generate_summary(decision, ats_score, semantic_analysis)\n",
    "        \n",
    "        # Create final report\n",
    "        report = FinalReport(\n",
    "            decision=decision,\n",
    "            ats_score=ats_score,\n",
    "            semantic_analysis=semantic_analysis,\n",
    "            format_issues=format_issues,\n",
    "            bias_flags=bias_flags,\n",
    "            feedback=feedback,\n",
    "            improvement_checklist=improvement_checklist,\n",
    "            summary=summary\n",
    "        )\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*70)\n",
    "        print(\"âœ… ANALYSIS COMPLETE\")\n",
    "        print(\"=\"*70)\n",
    "        \n",
    "        return report\n",
    "\n",
    "print(\"âœ… Orchestrator Agent initialized\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ“Š Report Generator & Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-14T00:14:50.615732Z",
     "iopub.status.busy": "2026-02-14T00:14:50.615373Z",
     "iopub.status.idle": "2026-02-14T00:14:50.631842Z",
     "shell.execute_reply": "2026-02-14T00:14:50.631147Z",
     "shell.execute_reply.started": "2026-02-14T00:14:50.615707Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Orchestrator Agent Fixed (PDF Safety Check + Enum Fix)\n"
     ]
    }
   ],
   "source": [
    "class OrchestratorAgent:\n",
    "    \"\"\"\n",
    "    Agent Type: Orchestrator / Manager Agent\n",
    "    Role: Coordinate all agents (Text + Visual) and produce final report\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        # 1. Text Analysis Agents\n",
    "        self.cv_parser = CVParsingAgent()\n",
    "        self.jd_analyzer = JobDescriptionAgent()\n",
    "        self.ats_scorer = ATSScoringAgent()\n",
    "        self.semantic_analyzer = SemanticSimilarityAgent()\n",
    "        self.format_validator = FormattingValidatorAgent()\n",
    "        self.bias_checker = BiasComplianceAgent()\n",
    "        self.feedback_agent = FeedbackAgent()\n",
    "        \n",
    "        # 2. Visual Analysis Agent\n",
    "        self.visual_agent = VisualLayoutAgent()\n",
    "    \n",
    "    def determine_decision(self, ats_score, semantic_analysis, format_issues, bias_flags, visual_report) -> Decision:\n",
    "        \"\"\"Determine final pass/borderline/reject decision\"\"\"\n",
    "        \n",
    "        # Automatic reject conditions\n",
    "        if not bias_flags.is_compliant:\n",
    "            return Decision.REJECT\n",
    "        \n",
    "        # Reject if skills are buried at the bottom (Visual Check - only if available)\n",
    "        if visual_report.get('skills_position', '').startswith('Bottom'):\n",
    "            return Decision.BORDERLINE\n",
    "        \n",
    "        score = ats_score.overall_score\n",
    "        \n",
    "        # Pass conditions\n",
    "        if (score >= 75 and semantic_analysis.cv_jd_similarity >= 0.6 and format_issues.ats_friendly):\n",
    "            return Decision.PASS\n",
    "        \n",
    "        # Borderline conditions\n",
    "        if (score >= 60 or semantic_analysis.cv_jd_similarity >= 0.5):\n",
    "            return Decision.BORDERLINE\n",
    "        \n",
    "        return Decision.REJECT\n",
    "    \n",
    "    def create_improvement_checklist(self, feedback, format_issues, bias_flags, visual_report) -> List[str]:\n",
    "        \"\"\"Create actionable improvement checklist combining ALL agents\"\"\"\n",
    "        checklist = []\n",
    "        \n",
    "        # 1. Content Feedback\n",
    "        checklist.extend(feedback.improvement_priority)\n",
    "        \n",
    "        # 2. Visual Layout Issues\n",
    "        if visual_report.get('issues'):\n",
    "            for issue in visual_report['issues']:\n",
    "                checklist.append(f\"ğŸ¨ {issue}\")\n",
    "        \n",
    "        # 3. Formatting\n",
    "        if not format_issues.ats_friendly:\n",
    "            checklist.extend([f\"ğŸŸ  {rec}\" for rec in format_issues.recommendations])\n",
    "        \n",
    "        # 4. Compliance\n",
    "        if not bias_flags.is_compliant:\n",
    "            checklist.extend([f\"ğŸ”´ {issue}\" for issue in bias_flags.compliance_issues])\n",
    "        \n",
    "        return checklist\n",
    "    \n",
    "    def process(self, cv_file_path: str, jd_text: str) -> FinalReport:\n",
    "        print(\"\\n\" + \"=\"*70)\n",
    "        print(\"ğŸš€ STARTING ATS MULTI-AGENT ANALYSIS\")\n",
    "        print(\"=\"*70 + \"\\n\")\n",
    "        \n",
    "        # Step 1: Parse CV\n",
    "        print(\"[1/9] ğŸ“„ Parsing CV...\")\n",
    "        parsed_cv = self.cv_parser.parse(cv_file_path)\n",
    "        \n",
    "        # Step 2: Analyze JD\n",
    "        print(\"[2/9] ğŸ“‹ Analyzing JD...\")\n",
    "        job_req = self.jd_analyzer.analyze(jd_text)\n",
    "        \n",
    "        # Step 3: ATS Scoring\n",
    "        print(\"[3/9] ğŸ“Š ATS Scoring...\")\n",
    "        ats_score = self.ats_scorer.score(parsed_cv, job_req)\n",
    "        \n",
    "        # Step 4: Semantic Analysis\n",
    "        print(\"[4/9] ğŸ§  Semantic Analysis...\")\n",
    "        semantic_analysis = self.semantic_analyzer.analyze(parsed_cv, job_req)\n",
    "        \n",
    "        # Step 5: Format Validation\n",
    "        print(\"[5/9] ğŸ“ Format Validation...\")\n",
    "        format_issues = self.format_validator.validate(cv_file_path, parsed_cv)\n",
    "        \n",
    "        # Step 6: Bias Check\n",
    "        print(\"[6/9] âš–ï¸  Compliance Check...\")\n",
    "        bias_flags = self.bias_checker.check_compliance(cv_file_path, parsed_cv)\n",
    "        \n",
    "        # Step 7: Feedback (Fluff & Content)\n",
    "        print(\"[7/9] ğŸ’¡ Generating Content Feedback...\")\n",
    "        feedback = self.feedback_agent.generate_feedback(parsed_cv, job_req, ats_score)\n",
    "\n",
    "        # Step 8: Visual Analysis (Safe Mode)\n",
    "        print(\"[8/9] ğŸ‘ï¸ Visual Layout Analysis...\")\n",
    "        visual_report = {\"skills_position\": \"Skipped (Not PDF)\", \"issues\": [], \"layout_score\": 100}\n",
    "        \n",
    "        if cv_file_path.lower().endswith(\".pdf\"):\n",
    "            try:\n",
    "                visual_report = self.visual_agent.analyze_layout(cv_file_path)\n",
    "            except Exception as e:\n",
    "                print(f\"      âš ï¸ Visual Agent failed gracefully: {e}\")\n",
    "        else:\n",
    "            print(\"      âš ï¸ Skipping Visual Analysis (File is not PDF)\")\n",
    "        \n",
    "        # Step 9: Make Decision & Report\n",
    "        print(\"[9/9] Finalizing Report...\")\n",
    "        decision = self.determine_decision(ats_score, semantic_analysis, format_issues, bias_flags, visual_report)\n",
    "        checklist = self.create_improvement_checklist(feedback, format_issues, bias_flags, visual_report)\n",
    "        \n",
    "        # Generate Summary String\n",
    "        # FIX: Added .value to access the string inside the Enum\n",
    "        summary = (f\"**{decision.value.upper()}** - Score: {ats_score.overall_score:.0f}/100. \"\n",
    "                   f\"Visual Layout: {visual_report.get('skills_position', 'Unknown')}. \"\n",
    "                   f\"Semantic Match: {semantic_analysis.cv_jd_similarity:.0%}\")\n",
    "\n",
    "        # Create final report object\n",
    "        report = FinalReport(\n",
    "            decision=decision.value, # Return string value for JSON serialization\n",
    "            ats_score=ats_score,\n",
    "            semantic_analysis=semantic_analysis,\n",
    "            format_issues=format_issues,\n",
    "            bias_flags=bias_flags,\n",
    "            feedback=feedback,\n",
    "            improvement_checklist=checklist,\n",
    "            summary=summary\n",
    "        )\n",
    "        \n",
    "        # Attach visual_report dynamically\n",
    "        report.visual_report = visual_report \n",
    "\n",
    "        print(\"\\n\" + \"=\"*70)\n",
    "        print(\"âœ… ANALYSIS COMPLETE\")\n",
    "        print(\"=\"*70)\n",
    "        \n",
    "        return report\n",
    "\n",
    "print(\"âœ… Orchestrator Agent Fixed (PDF Safety Check + Enum Fix)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ§ª Example Usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-14T00:14:50.632917Z",
     "iopub.status.busy": "2026-02-14T00:14:50.632714Z",
     "iopub.status.idle": "2026-02-14T00:14:50.647879Z",
     "shell.execute_reply": "2026-02-14T00:14:50.647290Z",
     "shell.execute_reply.started": "2026-02-14T00:14:50.632900Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Sample data created\n"
     ]
    }
   ],
   "source": [
    "# Create sample CV text file for testing\n",
    "sample_cv = \"\"\"John Doe\n",
    "Email: john.doe@email.com\n",
    "Phone: +1-555-0123\n",
    "\n",
    "SUMMARY\n",
    "Experienced Software Engineer with 5 years in machine learning and backend development.\n",
    "\n",
    "SKILLS\n",
    "Python, Java, Machine Learning, TensorFlow, PyTorch, SQL, AWS, Docker, Git\n",
    "\n",
    "EXPERIENCE\n",
    "\n",
    "Senior Software Engineer\n",
    "Tech Company Inc.\n",
    "2021 - Present\n",
    "â€¢ Developed machine learning models for customer segmentation\n",
    "â€¢ Implemented REST APIs using Python and Flask\n",
    "â€¢ Improved model accuracy by 15% through feature engineering\n",
    "â€¢ Led team of 3 junior developers\n",
    "\n",
    "Software Engineer\n",
    "StartUp Co.\n",
    "2019 - 2021\n",
    "â€¢ Built data pipelines processing 1M+ records daily\n",
    "â€¢ Deployed models to production using Docker and AWS\n",
    "â€¢ Collaborated with cross-functional teams\n",
    "\n",
    "EDUCATION\n",
    "\n",
    "Bachelor of Science in Computer Science\n",
    "State University\n",
    "2019\n",
    "GPA: 3.8/4.0\n",
    "\"\"\"\n",
    "\n",
    "# Save sample CV\n",
    "with open('sample_cv.txt', 'w') as f:\n",
    "    f.write(sample_cv)\n",
    "\n",
    "# Sample Job Description\n",
    "sample_jd = \"\"\"Machine Learning Engineer\n",
    "\n",
    "We are looking for an experienced Machine Learning Engineer to join our team.\n",
    "\n",
    "REQUIREMENTS:\n",
    "â€¢ 3+ years of experience in machine learning\n",
    "â€¢ Required: Python, TensorFlow or PyTorch, SQL\n",
    "â€¢ Required: Experience with cloud platforms (AWS, GCP, or Azure)\n",
    "â€¢ Preferred: Deep Learning, NLP, Computer Vision\n",
    "â€¢ Preferred: Docker, Kubernetes\n",
    "â€¢ Strong communication and teamwork skills\n",
    "\n",
    "RESPONSIBILITIES:\n",
    "â€¢ Design and implement machine learning models\n",
    "â€¢ Deploy models to production\n",
    "â€¢ Collaborate with data scientists and engineers\n",
    "â€¢ Optimize model performance\n",
    "\"\"\"\n",
    "\n",
    "print(\"âœ… Sample data created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-14T00:14:50.649116Z",
     "iopub.status.busy": "2026-02-14T00:14:50.648813Z",
     "iopub.status.idle": "2026-02-14T00:14:50.776048Z",
     "shell.execute_reply": "2026-02-14T00:14:50.775211Z",
     "shell.execute_reply.started": "2026-02-14T00:14:50.649094Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "ğŸš€ STARTING ATS MULTI-AGENT ANALYSIS\n",
      "======================================================================\n",
      "\n",
      "[1/9] ğŸ“„ Parsing CV...\n",
      "ğŸ” Parsing CV: sample_cv.txt\n",
      "âœ… Extracted 13 skills, 1 experiences, 1 education entries\n",
      "[2/9] ğŸ“‹ Analyzing JD...\n",
      "ğŸ” Analyzing Job Description with Semantic Agent...\n",
      "[3/9] ğŸ“Š ATS Scoring...\n",
      "ğŸ“Š Calculating ATS Score (Semantic Title Mode)...\n",
      "[4/9] ğŸ§  Semantic Analysis...\n",
      "ğŸ§  Performing Semantic Analysis...\n",
      "âœ… CV-JD Similarity: 61.30%\n",
      "âœ… Found 6 semantic skill matches\n",
      "âœ… Experience Relevance: 53.80%\n",
      "[5/9] ğŸ“ Format Validation...\n",
      "ğŸ” Checking ATS Compatibility...\n",
      "âœ… Resume is ATS-friendly!\n",
      "[6/9] âš–ï¸  Compliance Check...\n",
      "âš–ï¸  Checking Bias & Compliance...\n",
      "âœ… No compliance issues detected\n",
      "[7/9] ğŸ’¡ Generating Content Feedback...\n",
      "ğŸ’¡ Feedback Agent: Generating REAL content suggestions...\n",
      "[8/9] ğŸ‘ï¸ Visual Layout Analysis...\n",
      "      âš ï¸ Skipping Visual Analysis (File is not PDF)\n",
      "[9/9] Finalizing Report...\n",
      "\n",
      "======================================================================\n",
      "âœ… ANALYSIS COMPLETE\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# Initialize orchestrator\n",
    "orchestrator = OrchestratorAgent()\n",
    "\n",
    "# Process CV and JD\n",
    "report = orchestrator.process(\n",
    "    cv_file_path='sample_cv.txt',\n",
    "    jd_text=sample_jd\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-14T00:14:50.777579Z",
     "iopub.status.busy": "2026-02-14T00:14:50.777146Z",
     "iopub.status.idle": "2026-02-14T00:14:50.781356Z",
     "shell.execute_reply": "2026-02-14T00:14:50.780585Z",
     "shell.execute_reply.started": "2026-02-14T00:14:50.777538Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "ğŸ“‹ FINAL ATS ANALYSIS REPORT\n",
      "============================================================\n",
      "\n",
      "ğŸ”¹ DECISION: borderline\n",
      "ğŸ”¹ OVERALL SCORE: 57.6/100\n",
      "\n",
      "ğŸ“ SUMMARY:\n",
      "**BORDERLINE** - Score: 58/100. Visual Layout: Skipped (Not PDF). Semantic Match: 61%\n",
      "\n",
      "âš ï¸ MISSING CRITICAL SKILLS:\n",
      "azure, gcp\n",
      "\n",
      "âœ… IMPROVEMENT CHECKLIST:\n",
      "  - ğŸ”´ CRITICAL: Missing Keywords: azure, gcp\n",
      "  -    ğŸ’¡ Suggestion for 'azure': Add a bullet like -> \"Integrated azure into the development workflow to enhance system performance and scalability.\"\n",
      "  -    ğŸ’¡ Suggestion for 'gcp': Add a bullet like -> \"Integrated gcp into the development workflow to enhance system performance and scalability.\"\n",
      "\n",
      "ğŸ‘ï¸ VISUAL LAYOUT:\n",
      "  - Skills Position: Skipped (Not PDF)\n",
      "\n",
      "============================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Print final report\n",
    "print_final_report(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-14T00:14:50.782490Z",
     "iopub.status.busy": "2026-02-14T00:14:50.782222Z",
     "iopub.status.idle": "2026-02-14T00:14:50.795171Z",
     "shell.execute_reply": "2026-02-14T00:14:50.794370Z",
     "shell.execute_reply.started": "2026-02-14T00:14:50.782470Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Report successfully saved to: ats_analysis_report.json\n"
     ]
    }
   ],
   "source": [
    "# Save report as JSON\n",
    "save_report_json(report, 'ats_analysis_report.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸš€ Advanced Features & Extensions\n",
    "\n",
    "### Potential Enhancements:\n",
    "\n",
    "1. **LLM Integration** (Optional)\n",
    "   - Add Claude/GPT for better feedback generation\n",
    "   - Improve semantic understanding\n",
    "   \n",
    "2. **Database Integration**\n",
    "   - Store analysis results\n",
    "   - Track improvements over time\n",
    "   \n",
    "3. **API Wrapper**\n",
    "   - REST API for integration\n",
    "   - Batch processing\n",
    "   \n",
    "4. **Web Interface**\n",
    "   - Streamlit/Gradio UI\n",
    "   - Real-time analysis\n",
    "   \n",
    "5. **Industry-Specific Models**\n",
    "   - Tech vs Finance vs Healthcare\n",
    "   - Customized scoring weights\n",
    "   \n",
    "6. **Multi-Language Support**\n",
    "   - Parse CVs in different languages\n",
    "   - Translation capabilities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ“š Agent Communication Flow\n",
    "\n",
    "```\n",
    "CV (PDF/DOCX) â”€â”€â”€â”€â”€â”€â”\n",
    "                     â”‚\n",
    "                     â”œâ”€â”€â–º [1] CV Parsing Agent\n",
    "                     â”‚         â”‚\n",
    "Job Description â”€â”€â”€â”€â”€â”¤         â”‚\n",
    "                     â”‚         â”œâ”€â”€â–º [2] JD Understanding Agent\n",
    "                     â”‚         â”‚           â”‚\n",
    "                     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
    "                                           â”‚\n",
    "                        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "                        â”‚                                      â”‚\n",
    "                  [3] ATS Scoring              [4] Semantic Similarity\n",
    "                        â”‚                                      â”‚\n",
    "                        â”‚                                      â”‚\n",
    "                  [5] Format Validator          [6] Bias Checker\n",
    "                        â”‚                                      â”‚\n",
    "                        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "                                           â”‚\n",
    "                                  [7] Feedback Agent\n",
    "                                           â”‚\n",
    "                                  [8] Orchestrator\n",
    "                                           â”‚\n",
    "                                    Final Report\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ¯ Production Deployment Checklist\n",
    "\n",
    "- [ ] Add comprehensive error handling\n",
    "- [ ] Implement logging (structured logs)\n",
    "- [ ] Add monitoring and metrics\n",
    "- [ ] Create configuration management\n",
    "- [ ] Add authentication/authorization\n",
    "- [ ] Implement rate limiting\n",
    "- [ ] Add caching layer\n",
    "- [ ] Write comprehensive tests\n",
    "- [ ] Document API endpoints\n",
    "- [ ] Set up CI/CD pipeline\n",
    "- [ ] Add data privacy compliance (GDPR)\n",
    "- [ ] Implement audit logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-14T00:14:50.796480Z",
     "iopub.status.busy": "2026-02-14T00:14:50.796136Z",
     "iopub.status.idle": "2026-02-14T00:15:03.822527Z",
     "shell.execute_reply": "2026-02-14T00:15:03.821655Z",
     "shell.execute_reply.started": "2026-02-14T00:14:50.796452Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m10.5/10.5 MB\u001b[0m \u001b[31m83.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m:01\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m57.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install fastapi uvicorn pyngrok transformers==4.52.4 accelerate -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-14T00:15:03.824246Z",
     "iopub.status.busy": "2026-02-14T00:15:03.824017Z",
     "iopub.status.idle": "2026-02-14T00:15:03.827936Z",
     "shell.execute_reply": "2026-02-14T00:15:03.827267Z",
     "shell.execute_reply.started": "2026-02-14T00:15:03.824220Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "NGROK_TOKEN = \"39RPzOZHvRI8YLFxpzoasI6WUmI_2m9PMaphjvsyPctU5o2uj\"\n",
    "API_KEY = \"secret123\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-14T00:15:03.829352Z",
     "iopub.status.busy": "2026-02-14T00:15:03.829139Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 2.0/2.0 MB 17.6 MB/s eta 0:00:00\n",
      "\n",
      "ğŸ” Loading agents...\n",
      "âœ… All 7 agents loaded successfully!\n",
      "\n",
      "                                                                                                    \n",
      "======================================================================\n",
      "ğŸš€ SERVER ONLINE: https://abhorrently-threadless-reina.ngrok-free.dev\n",
      "======================================================================\n",
      "\n",
      "\n",
      "======================================================================\n",
      "ğŸš€ STARTING ANALYSIS\n",
      "======================================================================\n",
      "\n",
      "[1/7] ğŸ“„ Parsing CV...\n",
      "ğŸ” Parsing CV: /tmp/tmpwu_d8_5e.docx\n",
      "âœ… Extracted 12 skills, 0 experiences, 2 education entries\n",
      "[2/7] ğŸ“‹ Analyzing JD...\n",
      "ğŸ” Analyzing Job Description with Semantic Agent...\n",
      "[3/7] ğŸ“Š ATS Scoring...\n",
      "ğŸ“Š Calculating ATS Score (Semantic Title Mode)...\n",
      "[4/7] ğŸ§  Semantic Analysis...\n",
      "ğŸ§  Performing Semantic Analysis...\n",
      "âœ… CV-JD Similarity: 60.50%\n",
      "âœ… Found 5 semantic skill matches\n",
      "âœ… Experience Relevance: 0.00%\n",
      "[5/7] ğŸ“ Format Validation...\n",
      "ğŸ” Checking ATS Compatibility...\n",
      "âœ… Resume is ATS-friendly!\n",
      "[6/7] âš–ï¸  Compliance Check...\n",
      "âš–ï¸  Checking Bias & Compliance...\n",
      "âœ… No compliance issues detected\n",
      "[7/7] ğŸ’¡ Generating Feedback...\n",
      "ğŸ’¡ Feedback Agent: Generating REAL content suggestions...\n",
      "\n",
      "======================================================================\n",
      "ğŸš€ STARTING ANALYSIS\n",
      "======================================================================\n",
      "\n",
      "[1/7] ğŸ“„ Parsing CV...\n",
      "ğŸ” Parsing CV: /tmp/tmpraq5dzmc.docx\n",
      "âœ… Extracted 12 skills, 0 experiences, 2 education entries\n",
      "[2/7] ğŸ“‹ Analyzing JD...\n",
      "ğŸ” Analyzing Job Description with Semantic Agent...\n",
      "[3/7] ğŸ“Š ATS Scoring...\n",
      "ğŸ“Š Calculating ATS Score (Semantic Title Mode)...\n",
      "[4/7] ğŸ§  Semantic Analysis...\n",
      "ğŸ§  Performing Semantic Analysis...\n",
      "âœ… CV-JD Similarity: 61.70%\n",
      "âœ… Found 5 semantic skill matches\n",
      "âœ… Experience Relevance: 0.00%\n",
      "[5/7] ğŸ“ Format Validation...\n",
      "ğŸ” Checking ATS Compatibility...\n",
      "âœ… Resume is ATS-friendly!\n",
      "[6/7] âš–ï¸  Compliance Check...\n",
      "âš–ï¸  Checking Bias & Compliance...\n",
      "âœ… No compliance issues detected\n",
      "[7/7] ğŸ’¡ Generating Feedback...\n",
      "ğŸ’¡ Feedback Agent: Generating REAL content suggestions...\n",
      "\n",
      "======================================================================\n",
      "ğŸš€ STARTING ANALYSIS\n",
      "======================================================================\n",
      "\n",
      "[1/7] ğŸ“„ Parsing CV...\n",
      "ğŸ” Parsing CV: /tmp/tmp1jlzy0_5.docx\n",
      "âœ… Extracted 12 skills, 0 experiences, 2 education entries\n",
      "[2/7] ğŸ“‹ Analyzing JD...\n",
      "ğŸ” Analyzing Job Description with Semantic Agent...\n",
      "[3/7] ğŸ“Š ATS Scoring...\n",
      "ğŸ“Š Calculating ATS Score (Semantic Title Mode)...\n",
      "[4/7] ğŸ§  Semantic Analysis...\n",
      "ğŸ§  Performing Semantic Analysis...\n",
      "âœ… CV-JD Similarity: 63.00%\n",
      "âœ… Found 5 semantic skill matches\n",
      "âœ… Experience Relevance: 0.00%\n",
      "[5/7] ğŸ“ Format Validation...\n",
      "ğŸ” Checking ATS Compatibility...\n",
      "âœ… Resume is ATS-friendly!\n",
      "[6/7] âš–ï¸  Compliance Check...\n",
      "âš–ï¸  Checking Bias & Compliance...\n",
      "âœ… No compliance issues detected\n",
      "[7/7] ğŸ’¡ Generating Feedback...\n",
      "ğŸ’¡ Feedback Agent: Generating REAL content suggestions...\n",
      "\n",
      "======================================================================\n",
      "ğŸš€ STARTING ANALYSIS\n",
      "======================================================================\n",
      "\n",
      "[1/7] ğŸ“„ Parsing CV...\n",
      "ğŸ” Parsing CV: /tmp/tmpndar0oyx.docx\n",
      "âœ… Extracted 12 skills, 0 experiences, 2 education entries\n",
      "[2/7] ğŸ“‹ Analyzing JD...\n",
      "ğŸ” Analyzing Job Description with Semantic Agent...\n",
      "[3/7] ğŸ“Š ATS Scoring...\n",
      "ğŸ“Š Calculating ATS Score (Semantic Title Mode)...\n",
      "[4/7] ğŸ§  Semantic Analysis...\n",
      "ğŸ§  Performing Semantic Analysis...\n",
      "âœ… CV-JD Similarity: 63.00%\n",
      "âœ… Found 5 semantic skill matches\n",
      "âœ… Experience Relevance: 0.00%\n",
      "[5/7] ğŸ“ Format Validation...\n",
      "ğŸ” Checking ATS Compatibility...\n",
      "âœ… Resume is ATS-friendly!\n",
      "[6/7] âš–ï¸  Compliance Check...\n",
      "âš–ï¸  Checking Bias & Compliance...\n",
      "âœ… No compliance issues detected\n",
      "[7/7] ğŸ’¡ Generating Feedback...\n",
      "ğŸ’¡ Feedback Agent: Generating REAL content suggestions...\n",
      "\n",
      "======================================================================\n",
      "ğŸš€ STARTING ANALYSIS\n",
      "======================================================================\n",
      "\n",
      "[1/7] ğŸ“„ Parsing CV...\n",
      "ğŸ” Parsing CV: /tmp/tmp9i74e88z.docx\n",
      "âœ… Extracted 12 skills, 0 experiences, 2 education entries\n",
      "[2/7] ğŸ“‹ Analyzing JD...\n",
      "ğŸ” Analyzing Job Description with Semantic Agent...\n",
      "[3/7] ğŸ“Š ATS Scoring...\n",
      "ğŸ“Š Calculating ATS Score (Semantic Title Mode)...\n",
      "[4/7] ğŸ§  Semantic Analysis...\n",
      "ğŸ§  Performing Semantic Analysis...\n",
      "âœ… CV-JD Similarity: 60.90%\n",
      "âœ… Found 5 semantic skill matches\n",
      "âœ… Experience Relevance: 0.00%\n",
      "[5/7] ğŸ“ Format Validation...\n",
      "ğŸ” Checking ATS Compatibility...\n",
      "âœ… Resume is ATS-friendly!\n",
      "[6/7] âš–ï¸  Compliance Check...\n",
      "âš–ï¸  Checking Bias & Compliance...\n",
      "âœ… No compliance issues detected\n",
      "[7/7] ğŸ’¡ Generating Feedback...\n",
      "ğŸ’¡ Feedback Agent: Generating REAL content suggestions...\n",
      "\n",
      "======================================================================\n",
      "ğŸš€ STARTING ANALYSIS\n",
      "======================================================================\n",
      "\n",
      "[1/7] ğŸ“„ Parsing CV...\n",
      "ğŸ” Parsing CV: /tmp/tmp0t6jq1rw.docx\n",
      "âœ… Extracted 12 skills, 0 experiences, 2 education entries\n",
      "[2/7] ğŸ“‹ Analyzing JD...\n",
      "ğŸ” Analyzing Job Description with Semantic Agent...\n",
      "[3/7] ğŸ“Š ATS Scoring...\n",
      "ğŸ“Š Calculating ATS Score (Semantic Title Mode)...\n",
      "[4/7] ğŸ§  Semantic Analysis...\n",
      "ğŸ§  Performing Semantic Analysis...\n",
      "âœ… CV-JD Similarity: 60.90%\n",
      "âœ… Found 5 semantic skill matches\n",
      "âœ… Experience Relevance: 0.00%\n",
      "[5/7] ğŸ“ Format Validation...\n",
      "ğŸ” Checking ATS Compatibility...\n",
      "âœ… Resume is ATS-friendly!\n",
      "[6/7] âš–ï¸  Compliance Check...\n",
      "âš–ï¸  Checking Bias & Compliance...\n",
      "âœ… No compliance issues detected\n",
      "[7/7] ğŸ’¡ Generating Feedback...\n",
      "ğŸ’¡ Feedback Agent: Generating REAL content suggestions...\n",
      "\n",
      "======================================================================\n",
      "ğŸš€ STARTING ANALYSIS\n",
      "======================================================================\n",
      "\n",
      "[1/7] ğŸ“„ Parsing CV...\n",
      "ğŸ” Parsing CV: /tmp/tmp7wrymdji.docx\n",
      "âœ… Extracted 12 skills, 0 experiences, 2 education entries\n",
      "[2/7] ğŸ“‹ Analyzing JD...\n",
      "ğŸ” Analyzing Job Description with Semantic Agent...\n",
      "[3/7] ğŸ“Š ATS Scoring...\n",
      "ğŸ“Š Calculating ATS Score (Semantic Title Mode)...\n",
      "[4/7] ğŸ§  Semantic Analysis...\n",
      "ğŸ§  Performing Semantic Analysis...\n",
      "âœ… CV-JD Similarity: 64.30%\n",
      "âœ… Found 5 semantic skill matches\n",
      "âœ… Experience Relevance: 0.00%\n",
      "[5/7] ğŸ“ Format Validation...\n",
      "ğŸ” Checking ATS Compatibility...\n",
      "âœ… Resume is ATS-friendly!\n",
      "[6/7] âš–ï¸  Compliance Check...\n",
      "âš–ï¸  Checking Bias & Compliance...\n",
      "âœ… No compliance issues detected\n",
      "[7/7] ğŸ’¡ Generating Feedback...\n",
      "ğŸ’¡ Feedback Agent: Generating REAL content suggestions...\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# PRODUCTION VERSION - No PDF Generation\n",
    "# Replace Cell 19 with this code\n",
    "# ============================================================================\n",
    "\n",
    "import sys\n",
    "import subprocess\n",
    "import os\n",
    "import asyncio\n",
    "import tempfile\n",
    "from pathlib import Path\n",
    "from dataclasses import asdict\n",
    "from typing import Optional, List\n",
    "\n",
    "# 1. Install Dependencies\n",
    "def install(package):\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", package])\n",
    "\n",
    "packages = ['fastapi', 'uvicorn', 'pyngrok', 'python-multipart', 'nest-asyncio', 'reportlab']\n",
    "for pkg in packages:\n",
    "    try:\n",
    "        __import__(pkg.replace('-', '_'))\n",
    "    except ImportError:\n",
    "        install(pkg)\n",
    "\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "\n",
    "# 2. Imports\n",
    "from fastapi import FastAPI, File, UploadFile, HTTPException, Depends, Header, Form\n",
    "# from fastapi.responses import FileResponse # Removed download import\n",
    "from fastapi.middleware.cors import CORSMiddleware\n",
    "from pydantic import BaseModel\n",
    "import uvicorn\n",
    "from pyngrok import ngrok, conf\n",
    "\n",
    "# ============================================================================\n",
    "# LOAD AGENTS\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\nğŸ” Loading agents...\")\n",
    "try:\n",
    "    # Ensure all agents are loaded from previous cells\n",
    "    cv_parser = CVParsingAgent()\n",
    "    jd_analyzer = JobDescriptionAgent()\n",
    "    ats_scorer = ATSScoringAgent()\n",
    "    semantic_analyzer = SemanticSimilarityAgent()\n",
    "    format_validator = FormattingValidatorAgent()\n",
    "    bias_checker = BiasComplianceAgent()\n",
    "    feedback_agent = FeedbackAgent()\n",
    "    # cv_generator = CVGeneratorAgent() # Removed Generator Agent\n",
    "    print(\"âœ… All 7 agents loaded successfully!\\n\")\n",
    "except NameError as e:\n",
    "    print(f\"âŒ Error: {e}\")\n",
    "    print(\"âš ï¸  Please run the Agent Definition cells (Cells 4-15) first!\")\n",
    "    raise\n",
    "\n",
    "# ============================================================================\n",
    "# CONFIGURATION\n",
    "# ============================================================================\n",
    "\n",
    "NGROK_TOKEN = \"39RPzOZHvRI8YLFxpzoasI6WUmI_2m9PMaphjvsyPctU5o2uj\" \n",
    "API_KEY = \"secret123\"\n",
    "\n",
    "# ============================================================================\n",
    "# ORCHESTRATOR\n",
    "# ============================================================================\n",
    "\n",
    "class ProductionOrchestrator:\n",
    "    def __init__(self):\n",
    "        self.cv_parser = cv_parser\n",
    "        self.jd_analyzer = jd_analyzer\n",
    "        self.ats_scorer = ats_scorer\n",
    "        self.semantic_analyzer = semantic_analyzer\n",
    "        self.format_validator = format_validator\n",
    "        self.bias_checker = bias_checker\n",
    "        self.feedback_agent = feedback_agent\n",
    "        # self.cv_generator = cv_generator # Removed\n",
    "        \n",
    "    def process(self, cv_file_path: str, jd_text: str) -> dict:\n",
    "        print(f\"\\n{'='*70}\")\n",
    "        print(\"ğŸš€ STARTING ANALYSIS\")\n",
    "        print(f\"{'='*70}\\n\")\n",
    "        \n",
    "        # 1. Parse CV\n",
    "        print(\"[1/7] ğŸ“„ Parsing CV...\")\n",
    "        parsed_cv = self.cv_parser.parse(cv_file_path)\n",
    "        \n",
    "        # 2. Analyze JD\n",
    "        print(\"[2/7] ğŸ“‹ Analyzing JD...\")\n",
    "        job_req = self.jd_analyzer.analyze(jd_text)\n",
    "        \n",
    "        # 3. ATS Score\n",
    "        print(\"[3/7] ğŸ“Š ATS Scoring...\")\n",
    "        ats_score = self.ats_scorer.score(parsed_cv, job_req)\n",
    "        \n",
    "        # 4. Semantic Analysis\n",
    "        print(\"[4/7] ğŸ§  Semantic Analysis...\")\n",
    "        semantic_analysis = self.semantic_analyzer.analyze(parsed_cv, job_req)\n",
    "        \n",
    "        # 5. Format Validation\n",
    "        print(\"[5/7] ğŸ“ Format Validation...\")\n",
    "        format_issues = self.format_validator.validate(cv_file_path, parsed_cv)\n",
    "        \n",
    "        # 6. Compliance Check\n",
    "        print(\"[6/7] âš–ï¸  Compliance Check...\")\n",
    "        bias_flags = self.bias_checker.check_compliance(cv_file_path, parsed_cv)\n",
    "        \n",
    "        # 7. Feedback Generation\n",
    "        print(\"[7/7] ğŸ’¡ Generating Feedback...\")\n",
    "        feedback = self.feedback_agent.generate_feedback(parsed_cv, job_req, ats_score)\n",
    "        \n",
    "        # Decision Logic\n",
    "        score = ats_score.overall_score\n",
    "        if not bias_flags.is_compliant:\n",
    "            decision = Decision.REJECT\n",
    "        elif score >= 80:\n",
    "            decision = Decision.PASS\n",
    "        elif score >= 65:\n",
    "            decision = Decision.BORDERLINE\n",
    "        else:\n",
    "            decision = Decision.REJECT\n",
    "            \n",
    "        # Removed Step 8: Auto-Improvement (PDF Generation)\n",
    "\n",
    "        # Construct Response\n",
    "        checklist = list(feedback.improvement_priority)\n",
    "        if not format_issues.ats_friendly:\n",
    "            checklist.extend([f\"ğŸŸ  {r}\" for r in format_issues.recommendations])\n",
    "        \n",
    "        summary = (f\"**{decision.value.upper()}** - Score: {score:.0f}/100. \"\n",
    "                   f\"Semantic Match: {semantic_analysis.cv_jd_similarity:.0%}\")\n",
    "        \n",
    "        return {\n",
    "            \"decision\": decision.value,\n",
    "            \"overall_score\": round(score, 2),\n",
    "            \"summary\": summary,\n",
    "            \"ats_score\": asdict(ats_score),\n",
    "            \"semantic_analysis\": asdict(semantic_analysis),\n",
    "            \"format_issues\": asdict(format_issues),\n",
    "            \"bias_flags\": asdict(bias_flags),\n",
    "            \"feedback\": asdict(feedback),\n",
    "            \"improvement_checklist\": checklist\n",
    "        }\n",
    "\n",
    "orchestrator = ProductionOrchestrator()\n",
    "\n",
    "# ============================================================================\n",
    "# API DEFINITION\n",
    "# ============================================================================\n",
    "\n",
    "app = FastAPI(title=\"ATS AI System\")\n",
    "\n",
    "app.add_middleware(\n",
    "    CORSMiddleware,\n",
    "    allow_origins=[\"*\"],\n",
    "    allow_credentials=True,\n",
    "    allow_methods=[\"*\"],\n",
    "    allow_headers=[\"*\"],\n",
    ")\n",
    "\n",
    "class AnalysisRequest(BaseModel):\n",
    "    job_description: str\n",
    "    cv_text: Optional[str] = None\n",
    "\n",
    "class AnalysisResponse(BaseModel):\n",
    "    decision: str\n",
    "    overall_score: float\n",
    "    summary: str\n",
    "    ats_score: dict\n",
    "    semantic_analysis: dict\n",
    "    format_issues: dict\n",
    "    bias_flags: dict\n",
    "    feedback: dict\n",
    "    improvement_checklist: List[str]\n",
    "\n",
    "async def verify_api_key(authorization: str = Header(None)):\n",
    "    if not authorization or authorization != f\"Bearer {API_KEY}\":\n",
    "        raise HTTPException(status_code=401, detail=\"Invalid API key\")\n",
    "    return True\n",
    "\n",
    "@app.get(\"/\")\n",
    "async def root():\n",
    "    return {\"status\": \"online\", \"system\": \"ATS Multi-Agent\"}\n",
    "\n",
    "# Removed /api/download-cv endpoint\n",
    "\n",
    "@app.post(\"/api/analyze\", response_model=AnalysisResponse)\n",
    "async def analyze(request: AnalysisRequest, authenticated: bool = Depends(verify_api_key)):\n",
    "    try:\n",
    "        with tempfile.NamedTemporaryFile(mode='w', delete=False, suffix='.txt', encoding='utf-8') as tmp:\n",
    "            tmp.write(request.cv_text)\n",
    "            tmp_path = tmp.name\n",
    "        \n",
    "        try:\n",
    "            return orchestrator.process(tmp_path, request.job_description)\n",
    "        finally:\n",
    "            os.unlink(tmp_path)\n",
    "    except Exception as e:\n",
    "        raise HTTPException(status_code=500, detail=str(e))\n",
    "\n",
    "@app.post(\"/api/analyze-file\", response_model=AnalysisResponse)\n",
    "async def analyze_file(job_description: str = Form(...), cv_file: UploadFile = File(...), authenticated: bool = Depends(verify_api_key)):\n",
    "    try:\n",
    "        file_ext = Path(cv_file.filename).suffix.lower()\n",
    "        if file_ext not in ['.pdf', '.docx', '.txt']:\n",
    "            raise HTTPException(status_code=400, detail=\"Unsupported file type\")\n",
    "        \n",
    "        with tempfile.NamedTemporaryFile(delete=False, suffix=file_ext) as tmp:\n",
    "            content = await cv_file.read()\n",
    "            tmp.write(content)\n",
    "            tmp_path = tmp.name\n",
    "        \n",
    "        try:\n",
    "            return orchestrator.process(tmp_path, job_description)\n",
    "        finally:\n",
    "            os.unlink(tmp_path)\n",
    "    except Exception as e:\n",
    "        raise HTTPException(status_code=500, detail=str(e))\n",
    "\n",
    "# ============================================================================\n",
    "# SERVER STARTUP\n",
    "# ============================================================================\n",
    "\n",
    "async def run_server():\n",
    "    try:\n",
    "        conf.get_default().auth_token = NGROK_TOKEN\n",
    "        # Kill existing tunnels\n",
    "        tunnels = ngrok.get_tunnels()\n",
    "        for t in tunnels:\n",
    "            ngrok.disconnect(t.public_url)\n",
    "            \n",
    "        public_url = ngrok.connect(8000, bind_tls=True)\n",
    "        print(\"\\n\" + \"=\"*70)\n",
    "        print(f\"ğŸš€ SERVER ONLINE: {public_url.public_url}\")\n",
    "        print(\"=\"*70 + \"\\n\")\n",
    "        \n",
    "        config = uvicorn.Config(app, host=\"0.0.0.0\", port=8000, log_level=\"error\")\n",
    "        server = uvicorn.Server(config)\n",
    "        await server.serve()\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Server Error: {e}\")\n",
    "\n",
    "try:\n",
    "    loop = asyncio.get_event_loop()\n",
    "except RuntimeError:\n",
    "    loop = asyncio.new_event_loop()\n",
    "    asyncio.set_event_loop(loop)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    loop.run_until_complete(run_server())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [],
   "dockerImageVersionId": 31260,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
